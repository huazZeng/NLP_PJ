{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PJ6\n",
    "\n",
    "## data & model \n",
    "* dataset define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class GSM8KDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, prompt=\"Answer the following math problem:\", max_length=512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.prompt = prompt\n",
    "\n",
    "        # 预先对 prompt 进行编码\n",
    "        self.prompt_ids = tokenizer(self.prompt, padding='do_not_pad', truncation=True, max_length=self.max_length, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "        print(self.prompt_ids)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "\n",
    "        # 对问题进行tokenization，保持最大长度\n",
    "        inputs = self.tokenizer(question, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        # 拼接编码后的 prompt 和问题的 input_ids\n",
    "        input_ids = torch.cat((self.prompt_ids, inputs[\"input_ids\"].squeeze(0)), dim=0)\n",
    "        attention_mask = torch.cat((torch.ones_like(self.prompt_ids), inputs[\"attention_mask\"].squeeze(0)), dim=0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"answer\": answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since openai/gsm8k couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'main' at F:\\dataset\\openai___gsm8k\\main\\0.0.0\\e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu Nov 28 12:08:49 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\", cache_dir=\"F:/dataset\")\n",
    "test_ds = ds[\"test\"]\n",
    "train_ds = ds[\"train\"]\n",
    "\n",
    "# 创建一个设备对象\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前设备: {device}\")\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "cache_directory = \"F:/model\"  # 指定你想要的缓存目录\n",
    "\n",
    "# 加载模型和分词器\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device,\n",
    "    cache_dir=cache_directory  # 添加此行\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory,device_map=device)  # 添加此行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2610,   525,   264, 10950,  6888, 17847,    13,  1446,   686,   387,\n",
      "         2661,   264,  6888,  3491,   323,   498,   686,  4226,   432,    13,\n",
      "          715, 14582,   549, 41601,   685,  6088, 26111,   311,   220,    19,\n",
      "           23,   315,  1059,  4780,   304,  5813,    11,   323,  1221,  1340,\n",
      "         6088,  4279,   438,  1657, 26111,   304,  3217,    13,  2585,  1657,\n",
      "        26111,  1521, 41601,   685,  4559, 30055,   304,  5813,   323,  3217,\n",
      "         5267, 16141,   549, 41601,   685,  6088,   220,    19,    23,    14,\n",
      "           17,   284,  1115,    19,    23,    14,    17,    28,    17,    19,\n",
      "         2452,    17,    19, 26111,   304,  3217,    13, 41601,   685,  6088,\n",
      "          220,    19,    23,    10,    17,    19,   284,  1115,    19,    23,\n",
      "           10,    17,    19,    28,    22,    17,  2452,    22,    17, 26111,\n",
      "        30055,   304,  5813,   323,  3217,    13,  1096,  4226,   374,   220,\n",
      "           22,    17,   382, 14582,   549,  3555,   374,   279,  2629,   315,\n",
      "          279,  1156,   220,    16,    15,  1496,  5109,  5267, 16141,   549,\n",
      "          576,  2629,   315,   279,  1156,   220,    16,    15,  1496,  5109,\n",
      "          374,   220,    17,    10,    19,    10,    21,    10,    23,    10,\n",
      "           16,    15,    10,    16,    17,    10,    16,    19,    10,    16,\n",
      "           21,    10,    16,    23,    10,    17,    15,   284,  1115,    17,\n",
      "           10,    19,    10,    21,    10,    23,    10,    16,    15,    10,\n",
      "           16,    17,    10,    16,    19,    10,    16,    21,    10,    16,\n",
      "           23,    10,    17,    15,    28,    22,    23,  2452,    22,    23,\n",
      "           13,  1096,  4226,   374,   220,    22,    23,   382])\n",
      "torch.Size([1, 730])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m---> 48\u001b[0m     Question \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     51\u001b[0m     true_answers \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "prompt = '''You are a helpful math assistant. You will be given a math problem and you will answer it. \n",
    "Question : Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "Answer : Natalia sold 48/2 = <<48/2=24>>24 clips in May. Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May. This answer is 72.\n",
    "\n",
    "Question : What is the sum of the first 10 even numbers?\n",
    "Answer : The sum of the first 10 even numbers is 2+4+6+8+10+12+14+16+18+20 = <<2+4+6+8+10+12+14+16+18+20=78>>78. This answer is 78.\n",
    "\n",
    "'''\n",
    "dataset = GSM8KDataset(train_ds, tokenizer,prompt=prompt)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "ANS_RE = re.compile(r\"#### (\\-?[0-9\\.\\,]+)\")\n",
    "INVALID_ANS = \"[invalid]\"\n",
    "COMPLETE_RE= re.compile(r\"This answer is (\\-?[0-9\\.\\,]+)\")\n",
    "\n",
    "def extract_answer(completion):\n",
    "    match = ANS_RE.search(completion)\n",
    "    if match:\n",
    "        match_str = match.group(1).strip()\n",
    "        match_str = match_str.replace(\",\", \"\")\n",
    "        return match_str\n",
    "    else:\n",
    "        return INVALID_ANS\n",
    "    \n",
    "def is_correct(generated_answer, true_answer ):\n",
    "    generated_answer = extract_answer(generated_answer)\n",
    "    if generated_answer == INVALID_ANS :\n",
    "        return False\n",
    "    \n",
    "    return extract_answer(true_answer) == gt_answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 计算总的准确率\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 推理并评估准确率\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"].size())\n",
    "    Question = batch[\"input_ids\"].to(device)\n",
    "    \n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    true_answers = batch[\"answer\"]\n",
    "    \n",
    "    # 推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(Question, attention_mask=attention_mask, max_length=1024)\n",
    "    \n",
    "    # 解码生成的答案\n",
    "    generated_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # 检查生成的答案与真实答案是否匹配\n",
    "    for generated_answer, true_answer in zip(generated_answers, true_answers):\n",
    "        if evaluate_accuracy(generated_answer, true_answer):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT\n",
    "* 全参数微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "train_dataset = GSM8KDataset(ds['train'], tokenizer)\n",
    "test_dataset = GSM8KDataset(ds['test'], tokenizer)\n",
    "\n",
    "# 配置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=3,\n",
    "    eval_steps=3,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# 初始化Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 训练\n",
    "trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "trainer.save_model(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
