{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于LLM微调的数学推理任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T10:35:26.867290Z",
     "iopub.status.busy": "2024-12-10T10:35:26.866963Z",
     "iopub.status.idle": "2024-12-10T10:35:33.016671Z",
     "shell.execute_reply": "2024-12-10T10:35:33.016164Z",
     "shell.execute_reply.started": "2024-12-10T10:35:26.867269Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 18:35:31.212927: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-10 18:35:31.508406: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-10 18:35:32.472976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T10:35:33.017842Z",
     "iopub.status.busy": "2024-12-10T10:35:33.017445Z",
     "iopub.status.idle": "2024-12-10T10:35:33.023506Z",
     "shell.execute_reply": "2024-12-10T10:35:33.023059Z",
     "shell.execute_reply.started": "2024-12-10T10:35:33.017822Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 设置代理\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T11:19:39.748086Z",
     "iopub.status.busy": "2024-12-10T11:19:39.747744Z",
     "iopub.status.idle": "2024-12-10T11:19:42.587846Z",
     "shell.execute_reply": "2024-12-10T11:19:42.587339Z",
     "shell.execute_reply.started": "2024-12-10T11:19:39.748055Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "dataset =  MsDataset.load('modelscope/gsm8k', subset_name='main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T11:18:49.281445Z",
     "iopub.status.busy": "2024-12-10T11:18:49.281105Z",
     "iopub.status.idle": "2024-12-10T11:18:49.285547Z",
     "shell.execute_reply": "2024-12-10T11:18:49.285061Z",
     "shell.execute_reply.started": "2024-12-10T11:18:49.281425Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloader(prompt, batch_size=1):\n",
    "    def preprocess(dataset, shuffle=True):\n",
    "        def collate_fn(batch):\n",
    "            questions =  [[{\"role\": \"user\", \"content\": prompt.format(text=item['question'])}] for item in batch]\n",
    "            answers = [item[\"answer\"] for item in batch]\n",
    "            return questions, answers\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "    testset = dataset['test']\n",
    "    return preprocess(testset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T11:18:42.520583Z",
     "iopub.status.busy": "2024-12-10T11:18:42.520248Z",
     "iopub.status.idle": "2024-12-10T11:18:42.524627Z",
     "shell.execute_reply": "2024-12-10T11:18:42.524117Z",
     "shell.execute_reply.started": "2024-12-10T11:18:42.520557Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ANS_RE = re.compile(r\"#### (\\-?[0-9\\.\\,]+)\")\n",
    "INVALID_ANS = \"[invalid]\"\n",
    "\n",
    "def extract_answer(answer):\n",
    "    match = ANS_RE.search(answer)\n",
    "    if match:\n",
    "        match_str = match.group(1).strip()\n",
    "        match_str = match_str.replace(\",\", \"\")\n",
    "        return match_str\n",
    "    else:\n",
    "        return INVALID_ANS\n",
    "    \n",
    "def is_correct(reply, truth):\n",
    "    answer = extract_answer(truth)\n",
    "    assert answer != INVALID_ANS\n",
    "    return extract_answer(reply) == answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T11:18:40.864954Z",
     "iopub.status.busy": "2024-12-10T11:18:40.864598Z",
     "iopub.status.idle": "2024-12-10T11:18:40.871264Z",
     "shell.execute_reply": "2024-12-10T11:18:40.870588Z",
     "shell.execute_reply.started": "2024-12-10T11:18:40.864930Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def eval_process(model_name, tokenizer_name, prompt, batch_size=1, ratio=1):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        cache_dir = '/autodl-tmp/cache'\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    testloader = get_dataloader(prompt, batch_size)\n",
    "    answers = []\n",
    "    replies = []\n",
    "    # 以下过程参照huggingface上的qwen2.5模型示例\n",
    "    for idx, (texts, truths) in tqdm(enumerate(testloader), total=int(len(testloader) * ratio)):\n",
    "        if idx >= int(len(testloader) * ratio):\n",
    "            break\n",
    "        texts = [tokenizer.apply_chat_template(\n",
    "            text,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ) for text in texts]\n",
    "        model_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, padding_side='left').to(model.device)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=256\n",
    "        )\n",
    "\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        replies.extend(responses)\n",
    "        answers.extend(truths)\n",
    "    return replies,answers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T11:19:46.198286Z",
     "iopub.status.busy": "2024-12-10T11:19:46.197859Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/NLP_PJ-main/code\n",
      "Downloading Model to directory: /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] Creating symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct.\n",
      "[WARNING:modelscope] Failed to create symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct: [Errno 17] File exists: '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct' -> '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct'\n",
      " 96%|█████████▌| 158/165 [19:21<00:51,  7.41s/it]"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "model_name = \"./output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-1036\"\n",
    "tokenizer_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "prompt = '''You are a highly skilled mathematician capable of solving complex grade-school math problems step by step. Please read the problem carefully and solve it with clear reasoning. Follow these instructions:\n",
    "\n",
    "- Identify the known information and the question being asked.\n",
    "- Break the solution into logical steps, providing clear explanations for each.\n",
    "- Show all calculations and intermediate results.\n",
    "- Conclude with a final answer.\n",
    "\n",
    "Output your response in the following format:\n",
    "[Explanation and calculations]\n",
    "#### [Final numerical answer]  \n",
    "\n",
    "Here is the problem:\n",
    "{text}'''\n",
    "replies,answers=eval_process(model_name, tokenizer_name, prompt, batch_size=8, ratio=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T10:35:41.244603Z",
     "iopub.status.busy": "2024-12-10T10:35:41.244265Z",
     "iopub.status.idle": "2024-12-10T10:35:41.507684Z",
     "shell.execute_reply": "2024-12-10T10:35:41.506866Z",
     "shell.execute_reply.started": "2024-12-10T10:35:41.244582Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'replies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reply, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mreplies\u001b[49m, answers):\n\u001b[1;32m      5\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m is_correct(reply, answer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'replies' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "total = 0\n",
    "correct = 0\n",
    "for reply, answer in zip(replies, answers):\n",
    "    total += 1\n",
    "    correct += is_correct(reply, answer)\n",
    "    \n",
    "print(f\"正确率: {(correct / total) * 100: .2f}%\")\n",
    "with open('output.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for reply, answer in zip(replies, answers):\n",
    "        # 创建一个字典\n",
    "        data = {\n",
    "            \"reply\": reply,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "        # 将字典转换为 JSON 格式，并写入文件\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T10:35:44.778536Z",
     "iopub.status.busy": "2024-12-10T10:35:44.778216Z",
     "iopub.status.idle": "2024-12-10T10:35:49.352391Z",
     "shell.execute_reply": "2024-12-10T10:35:49.351878Z",
     "shell.execute_reply.started": "2024-12-10T10:35:44.778517Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Successfully registered `/usr/local/lib/python3.10/site-packages/swift/llm/data/dataset_info.json`\n"
     ]
    }
   ],
   "source": [
    "# 导入swift框架进行微调\n",
    "from swift.llm import (\n",
    "    DatasetName, InferArguments, ModelType, SftArguments,\n",
    "    infer_main, sft_main, app_ui_main\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T10:35:49.353420Z",
     "iopub.status.busy": "2024-12-10T10:35:49.353189Z",
     "iopub.status.idle": "2024-12-10T11:17:01.838426Z",
     "shell.execute_reply": "2024-12-10T11:17:01.837903Z",
     "shell.execute_reply.started": "2024-12-10T10:35:49.353402Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Setting template_type: qwen2_5\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 1\n",
      "[INFO:swift] output_dir: /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550\n",
      "[INFO:swift] Start time of running main: 2024-12-10 18:35:50.307628\n",
      "[INFO:swift] args: SftArguments(model_type='qwen2_5-0_5b-instruct', model_id_or_path='qwen/Qwen2.5-0.5B-Instruct', model_revision='master', full_determinism=False, sft_type='lora', freeze_parameters=[], freeze_vit=False, freeze_parameters_ratio=0.0, additional_trainable_parameters=[], tuner_backend='peft', template_type='qwen2_5', output_dir='/mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550', add_output_dir_suffix=True, ddp_backend=None, ddp_find_unused_parameters=None, ddp_broadcast_buffers=None, ddp_timeout=1800, seed=42, resume_from_checkpoint=None, resume_only_model=False, ignore_data_skip=False, dtype='bf16', packing=False, train_backend='transformers', tp=1, pp=1, min_lr=None, sequence_parallel=False, model_kwargs={}, loss_name=None, dataset=['gsm8k_argu.jsonl'], val_dataset=[], dataset_seed=42, dataset_test_ratio=0.01, use_loss_scale=False, loss_scale_config_path='/usr/local/lib/python3.10/site-packages/swift/llm/agent/default_loss_scale_config.json', system=None, tools_prompt='react_en', max_length=4096, truncation_strategy='delete', check_dataset_strategy='none', streaming=False, streaming_val_size=0, streaming_buffer_size=16384, model_name=[None, None], model_author=[None, None], quant_method=None, quantization_bit=0, hqq_axis=0, hqq_dynamic_config_path=None, bnb_4bit_comp_dtype='bf16', bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, rescale_image=-1, target_modules=['q_proj', 'k_proj', 'v_proj'], target_regex=None, modules_to_save=[], lora_rank=8, lora_alpha=32, lora_dropout=0.05, lora_bias_trainable='none', lora_dtype='AUTO', lora_lr_ratio=None, use_rslora=False, use_dora=False, init_lora_weights='true', fourier_n_frequency=2000, fourier_scaling=300.0, rope_scaling=None, boft_block_size=4, boft_block_num=0, boft_n_butterfly_factor=1, boft_dropout=0.0, vera_rank=256, vera_projection_prng_key=0, vera_dropout=0.0, vera_d_initial=0.1, adapter_act='gelu', adapter_length=128, use_galore=False, galore_target_modules=None, galore_rank=128, galore_update_proj_gap=50, galore_scale=1.0, galore_proj_type='std', galore_optim_per_parameter=False, galore_with_embedding=False, galore_quantization=False, galore_proj_quant=False, galore_proj_bits=4, galore_proj_group_size=256, galore_cos_threshold=0.4, galore_gamma_proj=2, galore_queue_size=5, adalora_target_r=8, adalora_init_r=12, adalora_tinit=0, adalora_tfinal=0, adalora_deltaT=1, adalora_beta1=0.85, adalora_beta2=0.85, adalora_orth_reg_weight=0.5, ia3_feedforward_modules=[], llamapro_num_new_blocks=4, llamapro_num_groups=None, neftune_noise_alpha=None, neftune_backend='transformers', lisa_activated_layers=0, lisa_step_interval=20, reft_layer_key=None, reft_layers=None, reft_rank=4, reft_intervention_type='LoreftIntervention', reft_args=None, use_liger=False, gradient_checkpointing=True, vit_use_gc=True, deepspeed=None, batch_size=1, eval_batch_size=1, auto_find_batch_size=False, num_train_epochs=1, max_steps=-1, optim='adamw_torch', adam_beta1=0.9, adam_beta2=0.95, adam_epsilon=1e-08, learning_rate=0.0001, weight_decay=0.1, gradient_accumulation_steps=16, max_grad_norm=1, predict_with_generate=False, lr_scheduler_type='cosine', lr_scheduler_kwargs={}, warmup_ratio=0.05, warmup_steps=0, eval_steps=50, save_steps=50, save_only_model=False, save_total_limit=2, logging_steps=5, acc_steps=1, dataloader_num_workers=1, dataloader_pin_memory=True, dataloader_drop_last=False, push_to_hub=False, hub_model_id=None, hub_token=None, hub_private_repo=False, hub_strategy='every_save', test_oom_error=False, disable_tqdm=False, lazy_tokenize=False, preprocess_num_proc=1, use_flash_attn=None, ignore_args_error=False, check_model_is_latest=True, logging_dir='/mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/runs', report_to=['tensorboard'], acc_strategy='token', save_on_each_node=False, evaluation_strategy='steps', save_strategy='steps', save_safetensors=True, gpu_memory_fraction=None, include_num_input_tokens_seen=False, local_repo_path=None, custom_register_path=None, custom_dataset_info=None, device_map_config=None, device_max_memory=[], max_new_tokens=2048, do_sample=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, fsdp='', fsdp_config=None, sequence_parallel_size=1, model_layer_cls_name=None, metric_warmup_step=0, fsdp_num=1, per_device_train_batch_size=None, per_device_eval_batch_size=None, eval_strategy=None, self_cognition_sample=0, train_dataset_mix_ratio=0.0, train_dataset_mix_ds=['ms-bench'], train_dataset_sample=-1, val_dataset_sample=None, safe_serialization=None, only_save_model=None, neftune_alpha=None, deepspeed_config_path=None, model_cache_dir=None, lora_dropout_p=None, lora_target_modules=[], lora_target_regex=None, lora_modules_to_save=[], boft_target_modules=[], boft_modules_to_save=[], vera_target_modules=[], vera_modules_to_save=[], ia3_target_modules=[], ia3_modules_to_save=[], custom_train_dataset_path=[], custom_val_dataset_path=[], device_map_config_path=None, push_hub_strategy=None)\n",
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] Downloading the model from ModelScope Hub, model_id: qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_count: 1\n",
      "rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "Downloading Model to directory: /mnt/workspace/.cache/modelscope/hub/qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING:modelscope] Using branch: master as version is unstable, use with caution\n",
      "[INFO:modelscope] Creating symbolic link /mnt/workspace/.cache/modelscope/hub/qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/qwen/Qwen2.5-0.5B-Instruct.\n",
      "[WARNING:modelscope] Failed to create symbolic link /mnt/workspace/.cache/modelscope/hub/qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/qwen/Qwen2.5-0.5B-Instruct: [Errno 17] File exists: '/mnt/workspace/.cache/modelscope/hub/qwen/Qwen2___5-0___5B-Instruct' -> '/mnt/workspace/.cache/modelscope/hub/qwen/Qwen2.5-0.5B-Instruct'\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/.cache/modelscope/hub/qwen/Qwen2___5-0___5B-Instruct\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "[INFO:swift] model.max_model_len: 32768\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_config: Qwen2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"/mnt/workspace/.cache/modelscope/hub/qwen/Qwen2___5-0___5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO:swift] Setting model.config.use_cache: False\n",
      "[INFO:swift] target_modules: ['q_proj', 'k_proj', 'v_proj']\n",
      "[INFO:swift] modules_to_save: []\n",
      "[INFO:swift] lora_config: get_wrapped_class.<locals>.PeftWrapper(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/workspace/.cache/modelscope/hub/qwen/Qwen2___5-0___5B-Instruct', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'v_proj', 'q_proj', 'k_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] [base_model.model.model.embed_tokens.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.gate_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.up_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.down_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.input_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.post_attention_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] ...\n",
      "[INFO:swift] PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 896)\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=896, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "              (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "              (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] PeftModelForCausalLM: 494.7700M Params (0.7373M Trainable [0.1490%]), 0.0008M Buffers.\n",
      "[INFO:swift] system: You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "[INFO:swift] args.lazy_tokenize: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf5723fcfcf435aa7843f06c56475c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3118a554f0994d9993e3897ecf95f4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7511fa66894544e0a6202683f8c5135d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 16577\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 167\n",
      "})\n",
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2610, 525, 264, 7548, 25530, 20976, 1103, 12875, 315, 21828, 6351, 11972, 34689, 6888, 5322, 3019, 553, 3019, 13, 5209, 1349, 279, 3491, 15516, 323, 11625, 432, 448, 2797, 32711, 13, 11112, 1493, 11221, 1447, 12, 64547, 279, 3881, 1995, 323, 279, 3405, 1660, 4588, 624, 12, 15623, 279, 6291, 1119, 19819, 7354, 11, 8241, 2797, 40841, 369, 1817, 624, 12, 6928, 678, 28117, 323, 28439, 3059, 624, 12, 1200, 857, 448, 264, 1590, 4226, 382, 5097, 697, 2033, 304, 279, 2701, 3561, 510, 58, 69769, 323, 28117, 921, 820, 400, 58, 19357, 34776, 4226, 94280, 4710, 8420, 374, 279, 3491, 510, 50, 361, 374, 20971, 504, 1532, 4261, 311, 5836, 12879, 11, 220, 17, 19, 4115, 2937, 1283, 20327, 304, 1532, 4261, 504, 1532, 25919, 13, 1416, 279, 11618, 504, 1532, 25919, 311, 1532, 4261, 3867, 220, 19, 14, 20, 3039, 438, 1753, 882, 438, 1340, 4990, 311, 5821, 504, 1532, 4261, 311, 5836, 12879, 11, 323, 1340, 11835, 304, 5836, 12879, 220, 18, 21, 4115, 2937, 1283, 80473, 504, 1532, 4261, 11, 11047, 279, 2790, 882, 1340, 3867, 311, 5545, 5836, 12879, 504, 1532, 25919, 30, 151645, 198, 151644, 77091, 198, 47649, 3867, 220, 17, 19, 4115, 311, 5821, 504, 1532, 25919, 311, 1532, 4261, 624, 785, 11618, 504, 1532, 25919, 311, 1532, 4261, 3867, 220, 19, 14, 20, 3039, 438, 1753, 882, 438, 1340, 4990, 311, 5821, 504, 1532, 4261, 311, 5836, 12879, 624, 2679, 1340, 4990, 400, 87, 3, 4115, 311, 5821, 504, 1532, 4261, 311, 5836, 12879, 11, 1221, 279, 11618, 504, 1532, 25919, 311, 1532, 4261, 3867, 400, 19, 14, 20, 87, 3, 4115, 624, 1654, 1414, 429, 400, 19, 14, 20, 87, 284, 220, 17, 19, 54876, 773, 582, 646, 11625, 369, 400, 87, 3, 553, 84192, 2176, 11067, 315, 279, 23606, 553, 400, 20, 14, 19, 3, 510, 78045, 87, 284, 1124, 37018, 90, 17, 19, 1124, 50853, 220, 20, 15170, 19, 92, 284, 220, 18, 15, 7110, 921, 4416, 47649, 4990, 220, 18, 15, 4115, 311, 5821, 504, 1532, 4261, 311, 5836, 12879, 624, 7941, 1083, 3867, 458, 5107, 220, 17, 19, 4115, 311, 5821, 504, 1532, 25919, 311, 1532, 4261, 624, 54815, 11, 279, 2790, 882, 1340, 3867, 311, 5545, 5836, 12879, 504, 1532, 25919, 374, 400, 17, 19, 488, 220, 18, 15, 284, 220, 20, 19, 3, 4115, 13, 715, 820, 220, 20, 19, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a highly skilled mathematician capable of solving complex grade-school math problems step by step. Please read the problem carefully and solve it with clear reasoning. Follow these instructions:\n",
      "\n",
      "- Identify the known information and the question being asked.\n",
      "- Break the solution into logical steps, providing clear explanations for each.\n",
      "- Show all calculations and intermediate results.\n",
      "- Conclude with a final answer.\n",
      "\n",
      "Output your response in the following format:\n",
      "[Explanation and calculations]\n",
      "#### $[Final numerical answer]$ \n",
      "\n",
      "Here is the problem:\n",
      "Sue is traveling from New York to San Francisco, 24 hours later after landing in New York from New Orleans. If the journey from New Orleans to New York took 4/5 times as much time as she takes to travel from New York to San Francisco, and she lands in San Francisco 36 hours later after departing from New York, calculate the total time she took to reach San Francisco from New Orleans?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " Sue took 24 hours to travel from New Orleans to New York.\n",
      "The journey from New Orleans to New York took 4/5 times as much time as she takes to travel from New York to San Francisco.\n",
      "If she takes $x$ hours to travel from New York to San Francisco, then the journey from New Orleans to New York took $4/5x$ hours.\n",
      "We know that $4/5x = 24$, so we can solve for $x$ by multiplying both sides of the equation by $5/4$:\n",
      "\\[x = \\frac{24 \\cdot 5}{4} = 30.\\]\n",
      "So Sue takes 30 hours to travel from New York to San Francisco.\n",
      "She also took an additional 24 hours to travel from New Orleans to New York.\n",
      "Therefore, the total time she took to reach San Francisco from New Orleans is $24 + 30 = 54$ hours. \n",
      "#### 54<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 47649, 3867, 220, 17, 19, 4115, 311, 5821, 504, 1532, 25919, 311, 1532, 4261, 624, 785, 11618, 504, 1532, 25919, 311, 1532, 4261, 3867, 220, 19, 14, 20, 3039, 438, 1753, 882, 438, 1340, 4990, 311, 5821, 504, 1532, 4261, 311, 5836, 12879, 624, 2679, 1340, 4990, 400, 87, 3, 4115, 311, 5821, 504, 1532, 4261, 311, 5836, 12879, 11, 1221, 279, 11618, 504, 1532, 25919, 311, 1532, 4261, 3867, 400, 19, 14, 20, 87, 3, 4115, 624, 1654, 1414, 429, 400, 19, 14, 20, 87, 284, 220, 17, 19, 54876, 773, 582, 646, 11625, 369, 400, 87, 3, 553, 84192, 2176, 11067, 315, 279, 23606, 553, 400, 20, 14, 19, 3, 510, 78045, 87, 284, 1124, 37018, 90, 17, 19, 1124, 50853, 220, 20, 15170, 19, 92, 284, 220, 18, 15, 7110, 921, 4416, 47649, 4990, 220, 18, 15, 4115, 311, 5821, 504, 1532, 4261, 311, 5836, 12879, 624, 7941, 1083, 3867, 458, 5107, 220, 17, 19, 4115, 311, 5821, 504, 1532, 25919, 311, 1532, 4261, 624, 54815, 11, 279, 2790, 882, 1340, 3867, 311, 5545, 5836, 12879, 504, 1532, 25919, 374, 400, 17, 19, 488, 220, 18, 15, 284, 220, 20, 19, 3, 4115, 13, 715, 820, 220, 20, 19, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 217] Sue took 24 hours to travel from New Orleans to New York.\n",
      "The journey from New Orleans to New York took 4/5 times as much time as she takes to travel from New York to San Francisco.\n",
      "If she takes $x$ hours to travel from New York to San Francisco, then the journey from New Orleans to New York took $4/5x$ hours.\n",
      "We know that $4/5x = 24$, so we can solve for $x$ by multiplying both sides of the equation by $5/4$:\n",
      "\\[x = \\frac{24 \\cdot 5}{4} = 30.\\]\n",
      "So Sue takes 30 hours to travel from New York to San Francisco.\n",
      "She also took an additional 24 hours to travel from New Orleans to New York.\n",
      "Therefore, the total time she took to reach San Francisco from New Orleans is $24 + 30 = 54$ hours. \n",
      "#### 54<|im_end|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6aba1777ec04f4aa2dc20fe2a3f7b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c17fcf150e84eb08fa4cf1a388de348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Dataset Token Length: 450.971768±112.348658, min=211.000000, max=1263.000000, size=16577\n",
      "[INFO:swift] Dataset Token Length: 463.682635±114.851067, min=289.000000, max=1136.000000, size=167\n",
      "[INFO:swift] training_args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': False, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "additional_saved_files=[],\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      ",\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=16,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "loss_name=None,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=50,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_sample=-1,\n",
      "train_sampler_random=True,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "/usr/local/lib/python3.10/site-packages/swift/trainers/mixin.py:93: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-10 18:36:19,186] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: 没有那个文件或目录\n",
      "[INFO:swift] The SftArguments will be saved in: /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/sft_args.json\n",
      "[INFO:swift] The Seq2SeqTrainingArguments will be saved in: /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/training_args.json\n",
      "[INFO:swift] The logging file will be saved in: /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/logging.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d7428727be41ba971aad41d04032da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.41678157, 'acc': 0.87952828, 'grad_norm': 1.34483695, 'learning_rate': 1.92e-06, 'memory(GiB)': 6.74, 'train_speed(iter/s)': 0.243885, 'epoch': 0.0, 'global_step/max_steps': '1/1036', 'percentage': '0.10%', 'elapsed_time': '3s', 'remaining_time': '1h 2m 49s'}\n",
      "{'loss': 0.45851797, 'acc': 0.87340879, 'grad_norm': 1.41975617, 'learning_rate': 9.62e-06, 'memory(GiB)': 10.33, 'train_speed(iter/s)': 0.389428, 'epoch': 0.0, 'global_step/max_steps': '5/1036', 'percentage': '0.48%', 'elapsed_time': '12s', 'remaining_time': '42m 32s'}\n",
      "{'loss': 0.45646229, 'acc': 0.87215242, 'grad_norm': 1.17985845, 'learning_rate': 1.923e-05, 'memory(GiB)': 13.35, 'train_speed(iter/s)': 0.419882, 'epoch': 0.01, 'global_step/max_steps': '10/1036', 'percentage': '0.97%', 'elapsed_time': '23s', 'remaining_time': '39m 56s'}\n",
      "{'loss': 0.46041842, 'acc': 0.86985598, 'grad_norm': 1.1107831, 'learning_rate': 2.885e-05, 'memory(GiB)': 13.35, 'train_speed(iter/s)': 0.430922, 'epoch': 0.01, 'global_step/max_steps': '15/1036', 'percentage': '1.45%', 'elapsed_time': '34s', 'remaining_time': '38m 58s'}\n",
      "{'loss': 0.42827907, 'acc': 0.87095881, 'grad_norm': 0.80834162, 'learning_rate': 3.846e-05, 'memory(GiB)': 16.59, 'train_speed(iter/s)': 0.437293, 'epoch': 0.02, 'global_step/max_steps': '20/1036', 'percentage': '1.93%', 'elapsed_time': '45s', 'remaining_time': '38m 20s'}\n",
      "{'loss': 0.40692339, 'acc': 0.87763891, 'grad_norm': 1.17001951, 'learning_rate': 4.808e-05, 'memory(GiB)': 16.59, 'train_speed(iter/s)': 0.441111, 'epoch': 0.02, 'global_step/max_steps': '25/1036', 'percentage': '2.41%', 'elapsed_time': '56s', 'remaining_time': '37m 53s'}\n",
      "{'loss': 0.35406096, 'acc': 0.89115047, 'grad_norm': 0.95171869, 'learning_rate': 5.769e-05, 'memory(GiB)': 16.59, 'train_speed(iter/s)': 0.443335, 'epoch': 0.03, 'global_step/max_steps': '30/1036', 'percentage': '2.90%', 'elapsed_time': '1m 7s', 'remaining_time': '37m 33s'}\n",
      "{'loss': 0.34728718, 'acc': 0.89508629, 'grad_norm': 0.70250195, 'learning_rate': 6.731e-05, 'memory(GiB)': 16.59, 'train_speed(iter/s)': 0.444813, 'epoch': 0.03, 'global_step/max_steps': '35/1036', 'percentage': '3.38%', 'elapsed_time': '1m 18s', 'remaining_time': '37m 17s'}\n",
      "{'loss': 0.34528611, 'acc': 0.89672298, 'grad_norm': 0.62714875, 'learning_rate': 7.692e-05, 'memory(GiB)': 16.59, 'train_speed(iter/s)': 0.446136, 'epoch': 0.04, 'global_step/max_steps': '40/1036', 'percentage': '3.86%', 'elapsed_time': '1m 29s', 'remaining_time': '37m 1s'}\n",
      "{'loss': 0.32441576, 'acc': 0.89608269, 'grad_norm': 0.57487214, 'learning_rate': 8.654e-05, 'memory(GiB)': 16.59, 'train_speed(iter/s)': 0.446248, 'epoch': 0.04, 'global_step/max_steps': '45/1036', 'percentage': '4.34%', 'elapsed_time': '1m 40s', 'remaining_time': '36m 50s'}\n",
      "{'loss': 0.32935722, 'acc': 0.89599752, 'grad_norm': 0.65825152, 'learning_rate': 9.615e-05, 'memory(GiB)': 16.59, 'train_speed(iter/s)': 0.446987, 'epoch': 0.05, 'global_step/max_steps': '50/1036', 'percentage': '4.83%', 'elapsed_time': '1m 51s', 'remaining_time': '36m 36s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428cffba17064d36b2182350abceb812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30634254, 'eval_acc': 0.89838427, 'eval_runtime': 6.2326, 'eval_samples_per_second': 26.795, 'eval_steps_per_second': 26.795, 'epoch': 0.05, 'global_step/max_steps': '50/1036', 'percentage': '4.83%', 'elapsed_time': '1m 57s', 'remaining_time': '38m 40s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.31737936, 'acc': 0.89907618, 'grad_norm': 0.65752351, 'learning_rate': 0.0001, 'memory(GiB)': 17.88, 'train_speed(iter/s)': 0.42391, 'epoch': 0.05, 'global_step/max_steps': '55/1036', 'percentage': '5.31%', 'elapsed_time': '2m 9s', 'remaining_time': '38m 25s'}\n",
      "{'loss': 0.31152492, 'acc': 0.90028934, 'grad_norm': 0.59855688, 'learning_rate': 9.998e-05, 'memory(GiB)': 17.88, 'train_speed(iter/s)': 0.426171, 'epoch': 0.06, 'global_step/max_steps': '60/1036', 'percentage': '5.79%', 'elapsed_time': '2m 20s', 'remaining_time': '38m 2s'}\n",
      "{'loss': 0.29090018, 'acc': 0.9039443, 'grad_norm': 0.69104922, 'learning_rate': 9.996e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.428091, 'epoch': 0.06, 'global_step/max_steps': '65/1036', 'percentage': '6.27%', 'elapsed_time': '2m 31s', 'remaining_time': '37m 41s'}\n",
      "{'loss': 0.31664357, 'acc': 0.90046825, 'grad_norm': 0.65859425, 'learning_rate': 9.992e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.4296, 'epoch': 0.07, 'global_step/max_steps': '70/1036', 'percentage': '6.76%', 'elapsed_time': '2m 42s', 'remaining_time': '37m 22s'}\n",
      "{'loss': 0.31674702, 'acc': 0.89929361, 'grad_norm': 0.67885542, 'learning_rate': 9.987e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.431009, 'epoch': 0.07, 'global_step/max_steps': '75/1036', 'percentage': '7.24%', 'elapsed_time': '2m 53s', 'remaining_time': '37m 3s'}\n",
      "{'loss': 0.30542471, 'acc': 0.90357656, 'grad_norm': 0.72769976, 'learning_rate': 9.98e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.432468, 'epoch': 0.08, 'global_step/max_steps': '80/1036', 'percentage': '7.72%', 'elapsed_time': '3m 4s', 'remaining_time': '36m 45s'}\n",
      "{'loss': 0.30452476, 'acc': 0.89827909, 'grad_norm': 0.7000553, 'learning_rate': 9.972e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.43338, 'epoch': 0.08, 'global_step/max_steps': '85/1036', 'percentage': '8.20%', 'elapsed_time': '3m 15s', 'remaining_time': '36m 29s'}\n",
      "{'loss': 0.28702402, 'acc': 0.90585995, 'grad_norm': 0.68538141, 'learning_rate': 9.963e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.434398, 'epoch': 0.09, 'global_step/max_steps': '90/1036', 'percentage': '8.69%', 'elapsed_time': '3m 26s', 'remaining_time': '36m 12s'}\n",
      "{'loss': 0.30324924, 'acc': 0.90075216, 'grad_norm': 0.738469, 'learning_rate': 9.953e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.435273, 'epoch': 0.09, 'global_step/max_steps': '95/1036', 'percentage': '9.17%', 'elapsed_time': '3m 37s', 'remaining_time': '35m 57s'}\n",
      "{'loss': 0.29583797, 'acc': 0.90236282, 'grad_norm': 0.7754519, 'learning_rate': 9.941e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.4358, 'epoch': 0.1, 'global_step/max_steps': '100/1036', 'percentage': '9.65%', 'elapsed_time': '3m 49s', 'remaining_time': '35m 43s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fce89237bc4be794b46a3a3a9e9230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27877209, 'eval_acc': 0.90471092, 'eval_runtime': 6.2853, 'eval_samples_per_second': 26.57, 'eval_steps_per_second': 26.57, 'epoch': 0.1, 'global_step/max_steps': '100/1036', 'percentage': '9.65%', 'elapsed_time': '3m 55s', 'remaining_time': '36m 42s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27710443, 'acc': 0.9089838, 'grad_norm': 0.64613211, 'learning_rate': 9.929e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.424536, 'epoch': 0.1, 'global_step/max_steps': '105/1036', 'percentage': '10.14%', 'elapsed_time': '4m 6s', 'remaining_time': '36m 28s'}\n",
      "{'loss': 0.30220003, 'acc': 0.90372877, 'grad_norm': 0.82525146, 'learning_rate': 9.915e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.425676, 'epoch': 0.11, 'global_step/max_steps': '110/1036', 'percentage': '10.62%', 'elapsed_time': '4m 17s', 'remaining_time': '36m 11s'}\n",
      "{'loss': 0.28621938, 'acc': 0.90563765, 'grad_norm': 0.65999222, 'learning_rate': 9.899e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.426791, 'epoch': 0.11, 'global_step/max_steps': '115/1036', 'percentage': '11.10%', 'elapsed_time': '4m 28s', 'remaining_time': '35m 54s'}\n",
      "{'loss': 0.30143294, 'acc': 0.89992561, 'grad_norm': 0.65717202, 'learning_rate': 9.883e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.42753, 'epoch': 0.12, 'global_step/max_steps': '120/1036', 'percentage': '11.58%', 'elapsed_time': '4m 40s', 'remaining_time': '35m 39s'}\n",
      "{'loss': 0.30693474, 'acc': 0.89919147, 'grad_norm': 0.64273864, 'learning_rate': 9.865e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.428502, 'epoch': 0.12, 'global_step/max_steps': '125/1036', 'percentage': '12.07%', 'elapsed_time': '4m 51s', 'remaining_time': '35m 22s'}\n",
      "{'loss': 0.28845773, 'acc': 0.90238333, 'grad_norm': 0.61318368, 'learning_rate': 9.846e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.429305, 'epoch': 0.13, 'global_step/max_steps': '130/1036', 'percentage': '12.55%', 'elapsed_time': '5m 2s', 'remaining_time': '35m 7s'}\n",
      "{'loss': 0.25434995, 'acc': 0.90642691, 'grad_norm': 0.74013484, 'learning_rate': 9.825e-05, 'memory(GiB)': 18.47, 'train_speed(iter/s)': 0.429973, 'epoch': 0.13, 'global_step/max_steps': '135/1036', 'percentage': '13.03%', 'elapsed_time': '5m 13s', 'remaining_time': '34m 52s'}\n",
      "{'loss': 0.2723413, 'acc': 0.90766306, 'grad_norm': 0.53560507, 'learning_rate': 9.804e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.430906, 'epoch': 0.14, 'global_step/max_steps': '140/1036', 'percentage': '13.51%', 'elapsed_time': '5m 24s', 'remaining_time': '34m 36s'}\n",
      "{'loss': 0.26269379, 'acc': 0.91462851, 'grad_norm': 0.6381821, 'learning_rate': 9.781e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.431559, 'epoch': 0.14, 'global_step/max_steps': '145/1036', 'percentage': '14.00%', 'elapsed_time': '5m 35s', 'remaining_time': '34m 21s'}\n",
      "{'loss': 0.28596663, 'acc': 0.90679855, 'grad_norm': 0.54597467, 'learning_rate': 9.757e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.432133, 'epoch': 0.14, 'global_step/max_steps': '150/1036', 'percentage': '14.48%', 'elapsed_time': '5m 46s', 'remaining_time': '34m 7s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa6e67308d8409293ebe53e178c2e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27113593, 'eval_acc': 0.90692525, 'eval_runtime': 6.3307, 'eval_samples_per_second': 26.379, 'eval_steps_per_second': 26.379, 'epoch': 0.14, 'global_step/max_steps': '150/1036', 'percentage': '14.48%', 'elapsed_time': '5m 52s', 'remaining_time': '34m 45s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27134783, 'acc': 0.90941591, 'grad_norm': 0.54998612, 'learning_rate': 9.732e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.424878, 'epoch': 0.15, 'global_step/max_steps': '155/1036', 'percentage': '14.96%', 'elapsed_time': '6m 4s', 'remaining_time': '34m 30s'}\n",
      "{'loss': 0.2773684, 'acc': 0.9093956, 'grad_norm': 0.62356275, 'learning_rate': 9.706e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.425662, 'epoch': 0.15, 'global_step/max_steps': '160/1036', 'percentage': '15.44%', 'elapsed_time': '6m 15s', 'remaining_time': '34m 15s'}\n",
      "{'loss': 0.26877441, 'acc': 0.91171074, 'grad_norm': 0.66410273, 'learning_rate': 9.678e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.426378, 'epoch': 0.16, 'global_step/max_steps': '165/1036', 'percentage': '15.93%', 'elapsed_time': '6m 26s', 'remaining_time': '34m 0s'}\n",
      "{'loss': 0.27448847, 'acc': 0.90895863, 'grad_norm': 0.73776561, 'learning_rate': 9.649e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.427127, 'epoch': 0.16, 'global_step/max_steps': '170/1036', 'percentage': '16.41%', 'elapsed_time': '6m 37s', 'remaining_time': '33m 45s'}\n",
      "{'loss': 0.29192398, 'acc': 0.90287752, 'grad_norm': 0.67182338, 'learning_rate': 9.619e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.42776, 'epoch': 0.17, 'global_step/max_steps': '175/1036', 'percentage': '16.89%', 'elapsed_time': '6m 48s', 'remaining_time': '33m 30s'}\n",
      "{'loss': 0.27608609, 'acc': 0.90989246, 'grad_norm': 0.58924365, 'learning_rate': 9.588e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.428467, 'epoch': 0.17, 'global_step/max_steps': '180/1036', 'percentage': '17.37%', 'elapsed_time': '6m 59s', 'remaining_time': '33m 15s'}\n",
      "{'loss': 0.29229772, 'acc': 0.90584412, 'grad_norm': 0.69137478, 'learning_rate': 9.556e-05, 'memory(GiB)': 19.06, 'train_speed(iter/s)': 0.429043, 'epoch': 0.18, 'global_step/max_steps': '185/1036', 'percentage': '17.86%', 'elapsed_time': '7m 10s', 'remaining_time': '33m 1s'}\n",
      "{'loss': 0.31536186, 'acc': 0.89753904, 'grad_norm': 0.70527714, 'learning_rate': 9.523e-05, 'memory(GiB)': 19.76, 'train_speed(iter/s)': 0.429486, 'epoch': 0.18, 'global_step/max_steps': '190/1036', 'percentage': '18.34%', 'elapsed_time': '7m 21s', 'remaining_time': '32m 47s'}\n",
      "{'loss': 0.26275225, 'acc': 0.91552496, 'grad_norm': 0.67063606, 'learning_rate': 9.488e-05, 'memory(GiB)': 3.62, 'train_speed(iter/s)': 0.43007, 'epoch': 0.19, 'global_step/max_steps': '195/1036', 'percentage': '18.82%', 'elapsed_time': '7m 32s', 'remaining_time': '32m 33s'}\n",
      "{'loss': 0.26804001, 'acc': 0.91260967, 'grad_norm': 0.65341681, 'learning_rate': 9.452e-05, 'memory(GiB)': 3.62, 'train_speed(iter/s)': 0.430642, 'epoch': 0.19, 'global_step/max_steps': '200/1036', 'percentage': '19.31%', 'elapsed_time': '7m 43s', 'remaining_time': '32m 19s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be34906ee656453496275b7a6675027c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2675167, 'eval_acc': 0.90882324, 'eval_runtime': 6.227, 'eval_samples_per_second': 26.819, 'eval_steps_per_second': 26.819, 'epoch': 0.19, 'global_step/max_steps': '200/1036', 'percentage': '19.31%', 'elapsed_time': '7m 50s', 'remaining_time': '32m 45s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27602994, 'acc': 0.90647516, 'grad_norm': 0.66254073, 'learning_rate': 9.415e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.425109, 'epoch': 0.2, 'global_step/max_steps': '205/1036', 'percentage': '19.79%', 'elapsed_time': '8m 1s', 'remaining_time': '32m 32s'}\n",
      "{'loss': 0.26964691, 'acc': 0.91181889, 'grad_norm': 0.67439979, 'learning_rate': 9.377e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.425577, 'epoch': 0.2, 'global_step/max_steps': '210/1036', 'percentage': '20.27%', 'elapsed_time': '8m 12s', 'remaining_time': '32m 19s'}\n",
      "{'loss': 0.28539004, 'acc': 0.90704527, 'grad_norm': 0.60386878, 'learning_rate': 9.338e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.426093, 'epoch': 0.21, 'global_step/max_steps': '215/1036', 'percentage': '20.75%', 'elapsed_time': '8m 24s', 'remaining_time': '32m 5s'}\n",
      "{'loss': 0.28035662, 'acc': 0.90665894, 'grad_norm': 0.62438631, 'learning_rate': 9.298e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.426622, 'epoch': 0.21, 'global_step/max_steps': '220/1036', 'percentage': '21.24%', 'elapsed_time': '8m 35s', 'remaining_time': '31m 50s'}\n",
      "{'loss': 0.26491895, 'acc': 0.91437244, 'grad_norm': 0.56679714, 'learning_rate': 9.257e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.427039, 'epoch': 0.22, 'global_step/max_steps': '225/1036', 'percentage': '21.72%', 'elapsed_time': '8m 46s', 'remaining_time': '31m 37s'}\n",
      "{'loss': 0.25478089, 'acc': 0.91350832, 'grad_norm': 0.64913207, 'learning_rate': 9.214e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.42745, 'epoch': 0.22, 'global_step/max_steps': '230/1036', 'percentage': '22.20%', 'elapsed_time': '8m 57s', 'remaining_time': '31m 23s'}\n",
      "{'loss': 0.26737161, 'acc': 0.91081581, 'grad_norm': 0.60207981, 'learning_rate': 9.171e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.427921, 'epoch': 0.23, 'global_step/max_steps': '235/1036', 'percentage': '22.68%', 'elapsed_time': '9m 8s', 'remaining_time': '31m 10s'}\n",
      "{'loss': 0.29190249, 'acc': 0.90809736, 'grad_norm': 0.67262661, 'learning_rate': 9.126e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.428251, 'epoch': 0.23, 'global_step/max_steps': '240/1036', 'percentage': '23.17%', 'elapsed_time': '9m 19s', 'remaining_time': '30m 57s'}\n",
      "{'loss': 0.292243, 'acc': 0.89853621, 'grad_norm': 0.61703062, 'learning_rate': 9.08e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.428624, 'epoch': 0.24, 'global_step/max_steps': '245/1036', 'percentage': '23.65%', 'elapsed_time': '9m 31s', 'remaining_time': '30m 43s'}\n",
      "{'loss': 0.26623189, 'acc': 0.91233435, 'grad_norm': 0.58687687, 'learning_rate': 9.034e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.429015, 'epoch': 0.24, 'global_step/max_steps': '250/1036', 'percentage': '24.13%', 'elapsed_time': '9m 42s', 'remaining_time': '30m 30s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8883466599c44c99b0544b2a43d0f648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2643221, 'eval_acc': 0.90901791, 'eval_runtime': 6.1323, 'eval_samples_per_second': 27.233, 'eval_steps_per_second': 27.233, 'epoch': 0.24, 'global_step/max_steps': '250/1036', 'percentage': '24.13%', 'elapsed_time': '9m 48s', 'remaining_time': '30m 49s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26075196, 'acc': 0.91685419, 'grad_norm': 0.62922651, 'learning_rate': 8.986e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.424784, 'epoch': 0.25, 'global_step/max_steps': '255/1036', 'percentage': '24.61%', 'elapsed_time': '9m 59s', 'remaining_time': '30m 37s'}\n",
      "{'loss': 0.27178073, 'acc': 0.91191292, 'grad_norm': 0.59889001, 'learning_rate': 8.937e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.425172, 'epoch': 0.25, 'global_step/max_steps': '260/1036', 'percentage': '25.10%', 'elapsed_time': '10m 11s', 'remaining_time': '30m 23s'}\n",
      "{'loss': 0.28363941, 'acc': 0.90723906, 'grad_norm': 0.62022406, 'learning_rate': 8.888e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.42569, 'epoch': 0.26, 'global_step/max_steps': '265/1036', 'percentage': '25.58%', 'elapsed_time': '10m 22s', 'remaining_time': '30m 9s'}\n",
      "{'loss': 0.26965928, 'acc': 0.909482, 'grad_norm': 0.60406715, 'learning_rate': 8.837e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.426137, 'epoch': 0.26, 'global_step/max_steps': '270/1036', 'percentage': '26.06%', 'elapsed_time': '10m 33s', 'remaining_time': '29m 56s'}\n",
      "{'loss': 0.27621799, 'acc': 0.90788507, 'grad_norm': 0.6581043, 'learning_rate': 8.785e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.426566, 'epoch': 0.27, 'global_step/max_steps': '275/1036', 'percentage': '26.54%', 'elapsed_time': '10m 44s', 'remaining_time': '29m 42s'}\n",
      "{'loss': 0.28187618, 'acc': 0.90903511, 'grad_norm': 0.64149272, 'learning_rate': 8.733e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.427028, 'epoch': 0.27, 'global_step/max_steps': '280/1036', 'percentage': '27.03%', 'elapsed_time': '10m 55s', 'remaining_time': '29m 29s'}\n",
      "{'loss': 0.26356201, 'acc': 0.91250134, 'grad_norm': 0.59489018, 'learning_rate': 8.679e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.427497, 'epoch': 0.28, 'global_step/max_steps': '285/1036', 'percentage': '27.51%', 'elapsed_time': '11m 6s', 'remaining_time': '29m 15s'}\n",
      "{'loss': 0.29069166, 'acc': 0.9075593, 'grad_norm': 0.6185497, 'learning_rate': 8.625e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.427769, 'epoch': 0.28, 'global_step/max_steps': '290/1036', 'percentage': '27.99%', 'elapsed_time': '11m 17s', 'remaining_time': '29m 2s'}\n",
      "{'loss': 0.28055806, 'acc': 0.9064887, 'grad_norm': 0.65905231, 'learning_rate': 8.569e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.428195, 'epoch': 0.28, 'global_step/max_steps': '295/1036', 'percentage': '28.47%', 'elapsed_time': '11m 28s', 'remaining_time': '28m 49s'}\n",
      "{'loss': 0.27915595, 'acc': 0.9094615, 'grad_norm': 0.61455137, 'learning_rate': 8.513e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.428533, 'epoch': 0.29, 'global_step/max_steps': '300/1036', 'percentage': '28.96%', 'elapsed_time': '11m 39s', 'remaining_time': '28m 36s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a8218fd7ba415a826266615b36cfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26272419, 'eval_acc': 0.90972357, 'eval_runtime': 6.2049, 'eval_samples_per_second': 26.914, 'eval_steps_per_second': 26.914, 'epoch': 0.29, 'global_step/max_steps': '300/1036', 'percentage': '28.96%', 'elapsed_time': '11m 45s', 'remaining_time': '28m 51s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.25563717, 'acc': 0.91467686, 'grad_norm': 0.6666137, 'learning_rate': 8.456e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.424956, 'epoch': 0.29, 'global_step/max_steps': '305/1036', 'percentage': '29.44%', 'elapsed_time': '11m 57s', 'remaining_time': '28m 39s'}\n",
      "{'loss': 0.26373963, 'acc': 0.91128054, 'grad_norm': 0.57130748, 'learning_rate': 8.398e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.425295, 'epoch': 0.3, 'global_step/max_steps': '310/1036', 'percentage': '29.92%', 'elapsed_time': '12m 8s', 'remaining_time': '28m 25s'}\n",
      "{'loss': 0.25056698, 'acc': 0.91608267, 'grad_norm': 0.60594678, 'learning_rate': 8.339e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.425684, 'epoch': 0.3, 'global_step/max_steps': '315/1036', 'percentage': '30.41%', 'elapsed_time': '12m 19s', 'remaining_time': '28m 12s'}\n",
      "{'loss': 0.29853368, 'acc': 0.90539074, 'grad_norm': 0.70589805, 'learning_rate': 8.279e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.426013, 'epoch': 0.31, 'global_step/max_steps': '320/1036', 'percentage': '30.89%', 'elapsed_time': '12m 30s', 'remaining_time': '27m 59s'}\n",
      "{'loss': 0.275493, 'acc': 0.91055946, 'grad_norm': 0.64954078, 'learning_rate': 8.218e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.426374, 'epoch': 0.31, 'global_step/max_steps': '325/1036', 'percentage': '31.37%', 'elapsed_time': '12m 41s', 'remaining_time': '27m 46s'}\n",
      "{'loss': 0.27034478, 'acc': 0.91110497, 'grad_norm': 0.64120841, 'learning_rate': 8.157e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.426679, 'epoch': 0.32, 'global_step/max_steps': '330/1036', 'percentage': '31.85%', 'elapsed_time': '12m 52s', 'remaining_time': '27m 33s'}\n",
      "{'loss': 0.25991385, 'acc': 0.91122322, 'grad_norm': 0.55811805, 'learning_rate': 8.094e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.427005, 'epoch': 0.32, 'global_step/max_steps': '335/1036', 'percentage': '32.34%', 'elapsed_time': '13m 4s', 'remaining_time': '27m 20s'}\n",
      "{'loss': 0.28339348, 'acc': 0.91155548, 'grad_norm': 0.59927762, 'learning_rate': 8.031e-05, 'memory(GiB)': 3.63, 'train_speed(iter/s)': 0.427427, 'epoch': 0.33, 'global_step/max_steps': '340/1036', 'percentage': '32.82%', 'elapsed_time': '13m 14s', 'remaining_time': '27m 7s'}\n",
      "{'loss': 0.26438487, 'acc': 0.91370773, 'grad_norm': 0.62172467, 'learning_rate': 7.967e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427867, 'epoch': 0.33, 'global_step/max_steps': '345/1036', 'percentage': '33.30%', 'elapsed_time': '13m 25s', 'remaining_time': '26m 54s'}\n",
      "{'loss': 0.28248782, 'acc': 0.90571222, 'grad_norm': 0.49985132, 'learning_rate': 7.903e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.428165, 'epoch': 0.34, 'global_step/max_steps': '350/1036', 'percentage': '33.78%', 'elapsed_time': '13m 36s', 'remaining_time': '26m 41s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad1f685aa1b4dd3b973937ca83343c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26097482, 'eval_acc': 0.9109159, 'eval_runtime': 6.282, 'eval_samples_per_second': 26.584, 'eval_steps_per_second': 26.584, 'epoch': 0.34, 'global_step/max_steps': '350/1036', 'percentage': '33.78%', 'elapsed_time': '13m 43s', 'remaining_time': '26m 53s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.25576043, 'acc': 0.91307058, 'grad_norm': 0.58574593, 'learning_rate': 7.837e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.424966, 'epoch': 0.34, 'global_step/max_steps': '355/1036', 'percentage': '34.27%', 'elapsed_time': '13m 54s', 'remaining_time': '26m 41s'}\n",
      "{'loss': 0.2770504, 'acc': 0.91059837, 'grad_norm': 0.65120184, 'learning_rate': 7.771e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425323, 'epoch': 0.35, 'global_step/max_steps': '360/1036', 'percentage': '34.75%', 'elapsed_time': '14m 5s', 'remaining_time': '26m 28s'}\n",
      "{'loss': 0.25726769, 'acc': 0.91465197, 'grad_norm': 0.64284033, 'learning_rate': 7.704e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425717, 'epoch': 0.35, 'global_step/max_steps': '365/1036', 'percentage': '35.23%', 'elapsed_time': '14m 16s', 'remaining_time': '26m 15s'}\n",
      "{'loss': 0.26971099, 'acc': 0.91146078, 'grad_norm': 0.74145007, 'learning_rate': 7.637e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426034, 'epoch': 0.36, 'global_step/max_steps': '370/1036', 'percentage': '35.71%', 'elapsed_time': '14m 28s', 'remaining_time': '26m 2s'}\n",
      "{'loss': 0.2694741, 'acc': 0.90723696, 'grad_norm': 0.61979175, 'learning_rate': 7.569e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42633, 'epoch': 0.36, 'global_step/max_steps': '375/1036', 'percentage': '36.20%', 'elapsed_time': '14m 39s', 'remaining_time': '25m 49s'}\n",
      "{'loss': 0.27981684, 'acc': 0.90683737, 'grad_norm': 0.62672472, 'learning_rate': 7.5e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426697, 'epoch': 0.37, 'global_step/max_steps': '380/1036', 'percentage': '36.68%', 'elapsed_time': '14m 50s', 'remaining_time': '25m 36s'}\n",
      "{'loss': 0.25352011, 'acc': 0.91483974, 'grad_norm': 0.61140412, 'learning_rate': 7.431e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426965, 'epoch': 0.37, 'global_step/max_steps': '385/1036', 'percentage': '37.16%', 'elapsed_time': '15m 1s', 'remaining_time': '25m 23s'}\n",
      "{'loss': 0.29177454, 'acc': 0.90334015, 'grad_norm': 0.61417669, 'learning_rate': 7.361e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427209, 'epoch': 0.38, 'global_step/max_steps': '390/1036', 'percentage': '37.64%', 'elapsed_time': '15m 12s', 'remaining_time': '25m 11s'}\n",
      "{'loss': 0.28115292, 'acc': 0.90936594, 'grad_norm': 0.58542526, 'learning_rate': 7.29e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427512, 'epoch': 0.38, 'global_step/max_steps': '395/1036', 'percentage': '38.13%', 'elapsed_time': '15m 23s', 'remaining_time': '24m 58s'}\n",
      "{'loss': 0.28061583, 'acc': 0.90769939, 'grad_norm': 0.65391272, 'learning_rate': 7.219e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427783, 'epoch': 0.39, 'global_step/max_steps': '400/1036', 'percentage': '38.61%', 'elapsed_time': '15m 34s', 'remaining_time': '24m 46s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbbafc8a7264722b3ba22a068cba15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25940335, 'eval_acc': 0.91074557, 'eval_runtime': 6.289, 'eval_samples_per_second': 26.554, 'eval_steps_per_second': 26.554, 'epoch': 0.39, 'global_step/max_steps': '400/1036', 'percentage': '38.61%', 'elapsed_time': '15m 40s', 'remaining_time': '24m 56s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.25466352, 'acc': 0.91545229, 'grad_norm': 0.59321761, 'learning_rate': 7.147e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.424998, 'epoch': 0.39, 'global_step/max_steps': '405/1036', 'percentage': '39.09%', 'elapsed_time': '15m 52s', 'remaining_time': '24m 43s'}\n",
      "{'loss': 0.28640213, 'acc': 0.90338135, 'grad_norm': 0.55144155, 'learning_rate': 7.074e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.4253, 'epoch': 0.4, 'global_step/max_steps': '410/1036', 'percentage': '39.58%', 'elapsed_time': '16m 3s', 'remaining_time': '24m 31s'}\n",
      "{'loss': 0.26871178, 'acc': 0.91293783, 'grad_norm': 0.68962097, 'learning_rate': 7.002e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425579, 'epoch': 0.4, 'global_step/max_steps': '415/1036', 'percentage': '40.06%', 'elapsed_time': '16m 14s', 'remaining_time': '24m 18s'}\n",
      "{'loss': 0.28322296, 'acc': 0.90414934, 'grad_norm': 0.70662355, 'learning_rate': 6.928e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425895, 'epoch': 0.41, 'global_step/max_steps': '420/1036', 'percentage': '40.54%', 'elapsed_time': '16m 25s', 'remaining_time': '24m 5s'}\n",
      "{'loss': 0.26233711, 'acc': 0.91048803, 'grad_norm': 0.6100173, 'learning_rate': 6.854e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426164, 'epoch': 0.41, 'global_step/max_steps': '425/1036', 'percentage': '41.02%', 'elapsed_time': '16m 36s', 'remaining_time': '23m 53s'}\n",
      "{'loss': 0.26169987, 'acc': 0.913241, 'grad_norm': 0.61558509, 'learning_rate': 6.78e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426483, 'epoch': 0.42, 'global_step/max_steps': '430/1036', 'percentage': '41.51%', 'elapsed_time': '16m 47s', 'remaining_time': '23m 40s'}\n",
      "{'loss': 0.26950073, 'acc': 0.90704222, 'grad_norm': 0.64393616, 'learning_rate': 6.705e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42676, 'epoch': 0.42, 'global_step/max_steps': '435/1036', 'percentage': '41.99%', 'elapsed_time': '16m 58s', 'remaining_time': '23m 27s'}\n",
      "{'loss': 0.27672625, 'acc': 0.9059535, 'grad_norm': 0.64717245, 'learning_rate': 6.63e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426957, 'epoch': 0.42, 'global_step/max_steps': '440/1036', 'percentage': '42.47%', 'elapsed_time': '17m 10s', 'remaining_time': '23m 15s'}\n",
      "{'loss': 0.2583746, 'acc': 0.9169837, 'grad_norm': 0.58619118, 'learning_rate': 6.554e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427212, 'epoch': 0.43, 'global_step/max_steps': '445/1036', 'percentage': '42.95%', 'elapsed_time': '17m 21s', 'remaining_time': '23m 2s'}\n",
      "{'loss': 0.25762379, 'acc': 0.91350851, 'grad_norm': 0.61276364, 'learning_rate': 6.478e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427478, 'epoch': 0.43, 'global_step/max_steps': '450/1036', 'percentage': '43.44%', 'elapsed_time': '17m 32s', 'remaining_time': '22m 50s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb25a03d8c454192908bdb544bf286bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25828826, 'eval_acc': 0.91137824, 'eval_runtime': 6.22, 'eval_samples_per_second': 26.849, 'eval_steps_per_second': 26.849, 'epoch': 0.43, 'global_step/max_steps': '450/1036', 'percentage': '43.44%', 'elapsed_time': '17m 38s', 'remaining_time': '22m 58s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26153514, 'acc': 0.91296597, 'grad_norm': 0.73253894, 'learning_rate': 6.402e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425055, 'epoch': 0.44, 'global_step/max_steps': '455/1036', 'percentage': '43.92%', 'elapsed_time': '17m 49s', 'remaining_time': '22m 46s'}\n",
      "{'loss': 0.26347485, 'acc': 0.91485863, 'grad_norm': 0.54952782, 'learning_rate': 6.325e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425313, 'epoch': 0.44, 'global_step/max_steps': '460/1036', 'percentage': '44.40%', 'elapsed_time': '18m 1s', 'remaining_time': '22m 33s'}\n",
      "{'loss': 0.26718264, 'acc': 0.91362591, 'grad_norm': 0.69882333, 'learning_rate': 6.248e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425606, 'epoch': 0.45, 'global_step/max_steps': '465/1036', 'percentage': '44.88%', 'elapsed_time': '18m 12s', 'remaining_time': '22m 21s'}\n",
      "{'loss': 0.2902611, 'acc': 0.90425711, 'grad_norm': 0.65781045, 'learning_rate': 6.17e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425864, 'epoch': 0.45, 'global_step/max_steps': '470/1036', 'percentage': '45.37%', 'elapsed_time': '18m 23s', 'remaining_time': '22m 8s'}\n",
      "{'loss': 0.27023785, 'acc': 0.91230602, 'grad_norm': 0.63161427, 'learning_rate': 6.093e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426084, 'epoch': 0.46, 'global_step/max_steps': '475/1036', 'percentage': '45.85%', 'elapsed_time': '18m 34s', 'remaining_time': '21m 56s'}\n",
      "{'loss': 0.27248151, 'acc': 0.91062002, 'grad_norm': 0.60420942, 'learning_rate': 6.015e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426333, 'epoch': 0.46, 'global_step/max_steps': '480/1036', 'percentage': '46.33%', 'elapsed_time': '18m 45s', 'remaining_time': '21m 43s'}\n",
      "{'loss': 0.28609977, 'acc': 0.90842762, 'grad_norm': 0.62329596, 'learning_rate': 5.936e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426555, 'epoch': 0.47, 'global_step/max_steps': '485/1036', 'percentage': '46.81%', 'elapsed_time': '18m 56s', 'remaining_time': '21m 31s'}\n",
      "{'loss': 0.27127228, 'acc': 0.90949297, 'grad_norm': 0.61815995, 'learning_rate': 5.858e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426789, 'epoch': 0.47, 'global_step/max_steps': '490/1036', 'percentage': '47.30%', 'elapsed_time': '19m 7s', 'remaining_time': '21m 18s'}\n",
      "{'loss': 0.28820262, 'acc': 0.9077528, 'grad_norm': 0.64969599, 'learning_rate': 5.779e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427048, 'epoch': 0.48, 'global_step/max_steps': '495/1036', 'percentage': '47.78%', 'elapsed_time': '19m 18s', 'remaining_time': '21m 6s'}\n",
      "{'loss': 0.27074509, 'acc': 0.91094418, 'grad_norm': 0.6601302, 'learning_rate': 5.7e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427229, 'epoch': 0.48, 'global_step/max_steps': '500/1036', 'percentage': '48.26%', 'elapsed_time': '19m 29s', 'remaining_time': '20m 54s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37dde04631548218ba4957ccef3dc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25670248, 'eval_acc': 0.91227857, 'eval_runtime': 6.2345, 'eval_samples_per_second': 26.786, 'eval_steps_per_second': 26.786, 'epoch': 0.48, 'global_step/max_steps': '500/1036', 'percentage': '48.26%', 'elapsed_time': '19m 36s', 'remaining_time': '21m 0s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27668571, 'acc': 0.90886221, 'grad_norm': 0.69033849, 'learning_rate': 5.621e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425082, 'epoch': 0.49, 'global_step/max_steps': '505/1036', 'percentage': '48.75%', 'elapsed_time': '19m 47s', 'remaining_time': '20m 48s'}\n",
      "{'loss': 0.25849392, 'acc': 0.91560936, 'grad_norm': 0.62282383, 'learning_rate': 5.542e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425336, 'epoch': 0.49, 'global_step/max_steps': '510/1036', 'percentage': '49.23%', 'elapsed_time': '19m 58s', 'remaining_time': '20m 36s'}\n",
      "{'loss': 0.25902047, 'acc': 0.91108446, 'grad_norm': 0.62531513, 'learning_rate': 5.462e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425534, 'epoch': 0.5, 'global_step/max_steps': '515/1036', 'percentage': '49.71%', 'elapsed_time': '20m 9s', 'remaining_time': '20m 23s'}\n",
      "{'loss': 0.28885307, 'acc': 0.90395641, 'grad_norm': 0.69703913, 'learning_rate': 5.383e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425795, 'epoch': 0.5, 'global_step/max_steps': '520/1036', 'percentage': '50.19%', 'elapsed_time': '20m 20s', 'remaining_time': '20m 11s'}\n",
      "{'loss': 0.25399973, 'acc': 0.91575842, 'grad_norm': 0.62882447, 'learning_rate': 5.303e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426042, 'epoch': 0.51, 'global_step/max_steps': '525/1036', 'percentage': '50.68%', 'elapsed_time': '20m 31s', 'remaining_time': '19m 58s'}\n",
      "{'loss': 0.25070577, 'acc': 0.91595545, 'grad_norm': 0.56370223, 'learning_rate': 5.223e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42628, 'epoch': 0.51, 'global_step/max_steps': '530/1036', 'percentage': '51.16%', 'elapsed_time': '20m 42s', 'remaining_time': '19m 46s'}\n",
      "{'loss': 0.26495602, 'acc': 0.91572008, 'grad_norm': 0.61424565, 'learning_rate': 5.144e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426541, 'epoch': 0.52, 'global_step/max_steps': '535/1036', 'percentage': '51.64%', 'elapsed_time': '20m 53s', 'remaining_time': '19m 34s'}\n",
      "{'loss': 0.25965588, 'acc': 0.91544962, 'grad_norm': 0.63759387, 'learning_rate': 5.064e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42682, 'epoch': 0.52, 'global_step/max_steps': '540/1036', 'percentage': '52.12%', 'elapsed_time': '21m 4s', 'remaining_time': '19m 21s'}\n",
      "{'loss': 0.27865553, 'acc': 0.90912304, 'grad_norm': 0.63453597, 'learning_rate': 4.984e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427049, 'epoch': 0.53, 'global_step/max_steps': '545/1036', 'percentage': '52.61%', 'elapsed_time': '21m 15s', 'remaining_time': '19m 9s'}\n",
      "{'loss': 0.27967227, 'acc': 0.91055536, 'grad_norm': 0.64854228, 'learning_rate': 4.904e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427252, 'epoch': 0.53, 'global_step/max_steps': '550/1036', 'percentage': '53.09%', 'elapsed_time': '21m 26s', 'remaining_time': '18m 57s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f0014f94db438b933b35826318e881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2559799, 'eval_acc': 0.9118649, 'eval_runtime': 6.2902, 'eval_samples_per_second': 26.549, 'eval_steps_per_second': 26.549, 'epoch': 0.53, 'global_step/max_steps': '550/1036', 'percentage': '53.09%', 'elapsed_time': '21m 33s', 'remaining_time': '19m 2s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.24564161, 'acc': 0.91751261, 'grad_norm': 0.59584951, 'learning_rate': 4.824e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425285, 'epoch': 0.54, 'global_step/max_steps': '555/1036', 'percentage': '53.57%', 'elapsed_time': '21m 44s', 'remaining_time': '18m 50s'}\n",
      "{'loss': 0.27355428, 'acc': 0.91000652, 'grad_norm': 0.70027626, 'learning_rate': 4.745e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425503, 'epoch': 0.54, 'global_step/max_steps': '560/1036', 'percentage': '54.05%', 'elapsed_time': '21m 55s', 'remaining_time': '18m 38s'}\n",
      "{'loss': 0.26249852, 'acc': 0.912115, 'grad_norm': 0.59661812, 'learning_rate': 4.665e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425786, 'epoch': 0.55, 'global_step/max_steps': '565/1036', 'percentage': '54.54%', 'elapsed_time': '22m 6s', 'remaining_time': '18m 25s'}\n",
      "{'loss': 0.28156905, 'acc': 0.90434952, 'grad_norm': 0.64457703, 'learning_rate': 4.585e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425986, 'epoch': 0.55, 'global_step/max_steps': '570/1036', 'percentage': '55.02%', 'elapsed_time': '22m 17s', 'remaining_time': '18m 13s'}\n",
      "{'loss': 0.25646152, 'acc': 0.918573, 'grad_norm': 0.64934635, 'learning_rate': 4.506e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426212, 'epoch': 0.55, 'global_step/max_steps': '575/1036', 'percentage': '55.50%', 'elapsed_time': '22m 28s', 'remaining_time': '18m 1s'}\n",
      "{'loss': 0.24245362, 'acc': 0.91890411, 'grad_norm': 0.58464384, 'learning_rate': 4.427e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426383, 'epoch': 0.56, 'global_step/max_steps': '580/1036', 'percentage': '55.98%', 'elapsed_time': '22m 39s', 'remaining_time': '17m 49s'}\n",
      "{'loss': 0.25199764, 'acc': 0.91850929, 'grad_norm': 0.56036866, 'learning_rate': 4.347e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426565, 'epoch': 0.56, 'global_step/max_steps': '585/1036', 'percentage': '56.47%', 'elapsed_time': '22m 50s', 'remaining_time': '17m 36s'}\n",
      "{'loss': 0.26212037, 'acc': 0.9147954, 'grad_norm': 0.66117978, 'learning_rate': 4.268e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42681, 'epoch': 0.57, 'global_step/max_steps': '590/1036', 'percentage': '56.95%', 'elapsed_time': '23m 1s', 'remaining_time': '17m 24s'}\n",
      "{'loss': 0.27438955, 'acc': 0.9125577, 'grad_norm': 0.62852758, 'learning_rate': 4.189e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426998, 'epoch': 0.57, 'global_step/max_steps': '595/1036', 'percentage': '57.43%', 'elapsed_time': '23m 12s', 'remaining_time': '17m 12s'}\n",
      "{'loss': 0.2476491, 'acc': 0.91934462, 'grad_norm': 0.66258866, 'learning_rate': 4.111e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427195, 'epoch': 0.58, 'global_step/max_steps': '600/1036', 'percentage': '57.92%', 'elapsed_time': '23m 24s', 'remaining_time': '17m 0s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0631c24f1f4deb9d95a3493c160ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25538817, 'eval_acc': 0.91242457, 'eval_runtime': 6.2783, 'eval_samples_per_second': 26.6, 'eval_steps_per_second': 26.6, 'epoch': 0.58, 'global_step/max_steps': '600/1036', 'percentage': '57.92%', 'elapsed_time': '23m 30s', 'remaining_time': '17m 4s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26313572, 'acc': 0.91695051, 'grad_norm': 0.63550949, 'learning_rate': 4.032e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425371, 'epoch': 0.58, 'global_step/max_steps': '605/1036', 'percentage': '58.40%', 'elapsed_time': '23m 41s', 'remaining_time': '16m 52s'}\n",
      "{'loss': 0.23998737, 'acc': 0.91652718, 'grad_norm': 0.66796732, 'learning_rate': 3.954e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425577, 'epoch': 0.59, 'global_step/max_steps': '610/1036', 'percentage': '58.88%', 'elapsed_time': '23m 52s', 'remaining_time': '16m 40s'}\n",
      "{'loss': 0.2801301, 'acc': 0.91185026, 'grad_norm': 0.62324572, 'learning_rate': 3.876e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425769, 'epoch': 0.59, 'global_step/max_steps': '615/1036', 'percentage': '59.36%', 'elapsed_time': '24m 3s', 'remaining_time': '16m 28s'}\n",
      "{'loss': 0.26304297, 'acc': 0.91196823, 'grad_norm': 0.62588733, 'learning_rate': 3.799e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425971, 'epoch': 0.6, 'global_step/max_steps': '620/1036', 'percentage': '59.85%', 'elapsed_time': '24m 15s', 'remaining_time': '16m 16s'}\n",
      "{'loss': 0.2650157, 'acc': 0.9123332, 'grad_norm': 0.52679789, 'learning_rate': 3.721e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426184, 'epoch': 0.6, 'global_step/max_steps': '625/1036', 'percentage': '60.33%', 'elapsed_time': '24m 26s', 'remaining_time': '16m 4s'}\n",
      "{'loss': 0.27766266, 'acc': 0.90949459, 'grad_norm': 0.65849239, 'learning_rate': 3.644e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426342, 'epoch': 0.61, 'global_step/max_steps': '630/1036', 'percentage': '60.81%', 'elapsed_time': '24m 37s', 'remaining_time': '15m 51s'}\n",
      "{'loss': 0.2678174, 'acc': 0.91184101, 'grad_norm': 0.79411221, 'learning_rate': 3.568e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426518, 'epoch': 0.61, 'global_step/max_steps': '635/1036', 'percentage': '61.29%', 'elapsed_time': '24m 48s', 'remaining_time': '15m 39s'}\n",
      "{'loss': 0.26261201, 'acc': 0.90885725, 'grad_norm': 0.58746028, 'learning_rate': 3.491e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426717, 'epoch': 0.62, 'global_step/max_steps': '640/1036', 'percentage': '61.78%', 'elapsed_time': '24m 59s', 'remaining_time': '15m 27s'}\n",
      "{'loss': 0.27491891, 'acc': 0.90977736, 'grad_norm': 0.59923971, 'learning_rate': 3.415e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426897, 'epoch': 0.62, 'global_step/max_steps': '645/1036', 'percentage': '62.26%', 'elapsed_time': '25m 10s', 'remaining_time': '15m 15s'}\n",
      "{'loss': 0.26252387, 'acc': 0.91267319, 'grad_norm': 0.63374627, 'learning_rate': 3.34e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.427136, 'epoch': 0.63, 'global_step/max_steps': '650/1036', 'percentage': '62.74%', 'elapsed_time': '25m 21s', 'remaining_time': '15m 3s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b138fa61f2a546eab9ac6915bbb829d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2545082, 'eval_acc': 0.91291123, 'eval_runtime': 6.1944, 'eval_samples_per_second': 26.96, 'eval_steps_per_second': 26.96, 'epoch': 0.63, 'global_step/max_steps': '650/1036', 'percentage': '62.74%', 'elapsed_time': '25m 27s', 'remaining_time': '15m 7s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.25566792, 'acc': 0.91471138, 'grad_norm': 0.60047388, 'learning_rate': 3.265e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425438, 'epoch': 0.63, 'global_step/max_steps': '655/1036', 'percentage': '63.22%', 'elapsed_time': '25m 39s', 'remaining_time': '14m 55s'}\n",
      "{'loss': 0.2514164, 'acc': 0.91725988, 'grad_norm': 0.57643515, 'learning_rate': 3.19e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42561, 'epoch': 0.64, 'global_step/max_steps': '660/1036', 'percentage': '63.71%', 'elapsed_time': '25m 50s', 'remaining_time': '14m 43s'}\n",
      "{'loss': 0.27669709, 'acc': 0.90805664, 'grad_norm': 0.6248703, 'learning_rate': 3.116e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425792, 'epoch': 0.64, 'global_step/max_steps': '665/1036', 'percentage': '64.19%', 'elapsed_time': '26m 1s', 'remaining_time': '14m 31s'}\n",
      "{'loss': 0.26978247, 'acc': 0.91109848, 'grad_norm': 0.59950215, 'learning_rate': 3.042e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425955, 'epoch': 0.65, 'global_step/max_steps': '670/1036', 'percentage': '64.67%', 'elapsed_time': '26m 12s', 'remaining_time': '14m 18s'}\n",
      "{'loss': 0.26877604, 'acc': 0.91318464, 'grad_norm': 0.62867069, 'learning_rate': 2.969e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426134, 'epoch': 0.65, 'global_step/max_steps': '675/1036', 'percentage': '65.15%', 'elapsed_time': '26m 23s', 'remaining_time': '14m 6s'}\n",
      "{'loss': 0.2599488, 'acc': 0.9168725, 'grad_norm': 0.62514538, 'learning_rate': 2.897e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426266, 'epoch': 0.66, 'global_step/max_steps': '680/1036', 'percentage': '65.64%', 'elapsed_time': '26m 34s', 'remaining_time': '13m 54s'}\n",
      "{'loss': 0.26805038, 'acc': 0.90987196, 'grad_norm': 0.70928401, 'learning_rate': 2.824e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426443, 'epoch': 0.66, 'global_step/max_steps': '685/1036', 'percentage': '66.12%', 'elapsed_time': '26m 45s', 'remaining_time': '13m 42s'}\n",
      "{'loss': 0.2860425, 'acc': 0.90659761, 'grad_norm': 0.60681897, 'learning_rate': 2.753e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426624, 'epoch': 0.67, 'global_step/max_steps': '690/1036', 'percentage': '66.60%', 'elapsed_time': '26m 56s', 'remaining_time': '13m 30s'}\n",
      "{'loss': 0.2687525, 'acc': 0.91059427, 'grad_norm': 0.56357461, 'learning_rate': 2.682e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426788, 'epoch': 0.67, 'global_step/max_steps': '695/1036', 'percentage': '67.08%', 'elapsed_time': '27m 7s', 'remaining_time': '13m 18s'}\n",
      "{'loss': 0.2500411, 'acc': 0.91904011, 'grad_norm': 0.57078242, 'learning_rate': 2.611e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426936, 'epoch': 0.68, 'global_step/max_steps': '700/1036', 'percentage': '67.57%', 'elapsed_time': '27m 19s', 'remaining_time': '13m 6s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e74e95e8aa4f2c8f8d9dd373e528bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25417382, 'eval_acc': 0.9128869, 'eval_runtime': 6.2736, 'eval_samples_per_second': 26.62, 'eval_steps_per_second': 26.62, 'epoch': 0.68, 'global_step/max_steps': '700/1036', 'percentage': '67.57%', 'elapsed_time': '27m 25s', 'remaining_time': '13m 9s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27866571, 'acc': 0.90642271, 'grad_norm': 0.73030114, 'learning_rate': 2.542e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425367, 'epoch': 0.68, 'global_step/max_steps': '705/1036', 'percentage': '68.05%', 'elapsed_time': '27m 36s', 'remaining_time': '12m 57s'}\n",
      "{'loss': 0.2871573, 'acc': 0.90657768, 'grad_norm': 0.60813868, 'learning_rate': 2.472e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42558, 'epoch': 0.69, 'global_step/max_steps': '710/1036', 'percentage': '68.53%', 'elapsed_time': '27m 47s', 'remaining_time': '12m 45s'}\n",
      "{'loss': 0.27289565, 'acc': 0.91299696, 'grad_norm': 0.6541605, 'learning_rate': 2.404e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425761, 'epoch': 0.69, 'global_step/max_steps': '715/1036', 'percentage': '69.02%', 'elapsed_time': '27m 58s', 'remaining_time': '12m 33s'}\n",
      "{'loss': 0.25463843, 'acc': 0.91465578, 'grad_norm': 0.57550502, 'learning_rate': 2.336e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425903, 'epoch': 0.69, 'global_step/max_steps': '720/1036', 'percentage': '69.50%', 'elapsed_time': '28m 10s', 'remaining_time': '12m 21s'}\n",
      "{'loss': 0.27213464, 'acc': 0.90988674, 'grad_norm': 0.85380906, 'learning_rate': 2.269e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426109, 'epoch': 0.7, 'global_step/max_steps': '725/1036', 'percentage': '69.98%', 'elapsed_time': '28m 20s', 'remaining_time': '12m 9s'}\n",
      "{'loss': 0.25369999, 'acc': 0.91340141, 'grad_norm': 0.60150546, 'learning_rate': 2.202e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426284, 'epoch': 0.7, 'global_step/max_steps': '730/1036', 'percentage': '70.46%', 'elapsed_time': '28m 32s', 'remaining_time': '11m 57s'}\n",
      "{'loss': 0.28741088, 'acc': 0.91157217, 'grad_norm': 0.60412723, 'learning_rate': 2.136e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426414, 'epoch': 0.71, 'global_step/max_steps': '735/1036', 'percentage': '70.95%', 'elapsed_time': '28m 43s', 'remaining_time': '11m 45s'}\n",
      "{'loss': 0.27536092, 'acc': 0.90915775, 'grad_norm': 0.61426044, 'learning_rate': 2.071e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42657, 'epoch': 0.71, 'global_step/max_steps': '740/1036', 'percentage': '71.43%', 'elapsed_time': '28m 54s', 'remaining_time': '11m 33s'}\n",
      "{'loss': 0.26955805, 'acc': 0.90723619, 'grad_norm': 0.62906027, 'learning_rate': 2.007e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426731, 'epoch': 0.72, 'global_step/max_steps': '745/1036', 'percentage': '71.91%', 'elapsed_time': '29m 5s', 'remaining_time': '11m 21s'}\n",
      "{'loss': 0.27371061, 'acc': 0.91297264, 'grad_norm': 0.64257437, 'learning_rate': 1.944e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426898, 'epoch': 0.72, 'global_step/max_steps': '750/1036', 'percentage': '72.39%', 'elapsed_time': '29m 16s', 'remaining_time': '11m 9s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f33cd13f06142c99d6b139dbec4a0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2540431, 'eval_acc': 0.91261923, 'eval_runtime': 6.2006, 'eval_samples_per_second': 26.933, 'eval_steps_per_second': 26.933, 'epoch': 0.72, 'global_step/max_steps': '750/1036', 'percentage': '72.39%', 'elapsed_time': '29m 22s', 'remaining_time': '11m 12s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.24436657, 'acc': 0.91904373, 'grad_norm': 0.59656858, 'learning_rate': 1.881e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425426, 'epoch': 0.73, 'global_step/max_steps': '755/1036', 'percentage': '72.88%', 'elapsed_time': '29m 34s', 'remaining_time': '11m 0s'}\n",
      "{'loss': 0.25207586, 'acc': 0.91481285, 'grad_norm': 0.61971325, 'learning_rate': 1.819e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42559, 'epoch': 0.73, 'global_step/max_steps': '760/1036', 'percentage': '73.36%', 'elapsed_time': '29m 45s', 'remaining_time': '10m 48s'}\n",
      "{'loss': 0.239222, 'acc': 0.92457199, 'grad_norm': 0.58013374, 'learning_rate': 1.758e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42578, 'epoch': 0.74, 'global_step/max_steps': '765/1036', 'percentage': '73.84%', 'elapsed_time': '29m 56s', 'remaining_time': '10m 36s'}\n",
      "{'loss': 0.24181275, 'acc': 0.92181196, 'grad_norm': 0.55514354, 'learning_rate': 1.697e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425951, 'epoch': 0.74, 'global_step/max_steps': '770/1036', 'percentage': '74.32%', 'elapsed_time': '30m 7s', 'remaining_time': '10m 24s'}\n",
      "{'loss': 0.25346062, 'acc': 0.91662331, 'grad_norm': 0.63066679, 'learning_rate': 1.638e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426071, 'epoch': 0.75, 'global_step/max_steps': '775/1036', 'percentage': '74.81%', 'elapsed_time': '30m 18s', 'remaining_time': '10m 12s'}\n",
      "{'loss': 0.28273652, 'acc': 0.90756493, 'grad_norm': 0.74331743, 'learning_rate': 1.579e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426239, 'epoch': 0.75, 'global_step/max_steps': '780/1036', 'percentage': '75.29%', 'elapsed_time': '30m 29s', 'remaining_time': '10m 0s'}\n",
      "{'loss': 0.27587309, 'acc': 0.90787544, 'grad_norm': 0.62876749, 'learning_rate': 1.521e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426393, 'epoch': 0.76, 'global_step/max_steps': '785/1036', 'percentage': '75.77%', 'elapsed_time': '30m 40s', 'remaining_time': '9m 48s'}\n",
      "{'loss': 0.28816369, 'acc': 0.91010637, 'grad_norm': 0.69064844, 'learning_rate': 1.464e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426531, 'epoch': 0.76, 'global_step/max_steps': '790/1036', 'percentage': '76.25%', 'elapsed_time': '30m 51s', 'remaining_time': '9m 36s'}\n",
      "{'loss': 0.25063291, 'acc': 0.91440582, 'grad_norm': 0.72398633, 'learning_rate': 1.408e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426684, 'epoch': 0.77, 'global_step/max_steps': '795/1036', 'percentage': '76.74%', 'elapsed_time': '31m 2s', 'remaining_time': '9m 24s'}\n",
      "{'loss': 0.25276508, 'acc': 0.91737528, 'grad_norm': 0.64009303, 'learning_rate': 1.353e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426819, 'epoch': 0.77, 'global_step/max_steps': '800/1036', 'percentage': '77.22%', 'elapsed_time': '31m 13s', 'remaining_time': '9m 12s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3427d5deedc4031b59227cdd994351d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25343505, 'eval_acc': 0.91257057, 'eval_runtime': 6.2731, 'eval_samples_per_second': 26.621, 'eval_steps_per_second': 26.621, 'epoch': 0.77, 'global_step/max_steps': '800/1036', 'percentage': '77.22%', 'elapsed_time': '31m 20s', 'remaining_time': '9m 14s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26282282, 'acc': 0.91148987, 'grad_norm': 0.6747883, 'learning_rate': 1.299e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425426, 'epoch': 0.78, 'global_step/max_steps': '805/1036', 'percentage': '77.70%', 'elapsed_time': '31m 31s', 'remaining_time': '9m 2s'}\n",
      "{'loss': 0.2708426, 'acc': 0.91258125, 'grad_norm': 0.63044292, 'learning_rate': 1.246e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425561, 'epoch': 0.78, 'global_step/max_steps': '810/1036', 'percentage': '78.19%', 'elapsed_time': '31m 42s', 'remaining_time': '8m 50s'}\n",
      "{'loss': 0.26263416, 'acc': 0.91244764, 'grad_norm': 0.60913837, 'learning_rate': 1.194e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425701, 'epoch': 0.79, 'global_step/max_steps': '815/1036', 'percentage': '78.67%', 'elapsed_time': '31m 54s', 'remaining_time': '8m 39s'}\n",
      "{'loss': 0.28103054, 'acc': 0.90986147, 'grad_norm': 0.64055115, 'learning_rate': 1.143e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425877, 'epoch': 0.79, 'global_step/max_steps': '820/1036', 'percentage': '79.15%', 'elapsed_time': '32m 4s', 'remaining_time': '8m 27s'}\n",
      "{'loss': 0.25059068, 'acc': 0.92107058, 'grad_norm': 0.73123181, 'learning_rate': 1.092e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42601, 'epoch': 0.8, 'global_step/max_steps': '825/1036', 'percentage': '79.63%', 'elapsed_time': '32m 16s', 'remaining_time': '8m 15s'}\n",
      "{'loss': 0.26481698, 'acc': 0.91226912, 'grad_norm': 0.57934457, 'learning_rate': 1.043e-05, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426153, 'epoch': 0.8, 'global_step/max_steps': '830/1036', 'percentage': '80.12%', 'elapsed_time': '32m 27s', 'remaining_time': '8m 3s'}\n",
      "{'loss': 0.2592315, 'acc': 0.91545029, 'grad_norm': 0.75673109, 'learning_rate': 9.95e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426292, 'epoch': 0.81, 'global_step/max_steps': '835/1036', 'percentage': '80.60%', 'elapsed_time': '32m 38s', 'remaining_time': '7m 51s'}\n",
      "{'loss': 0.25158918, 'acc': 0.91579199, 'grad_norm': 0.56692213, 'learning_rate': 9.47e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426435, 'epoch': 0.81, 'global_step/max_steps': '840/1036', 'percentage': '81.08%', 'elapsed_time': '32m 49s', 'remaining_time': '7m 39s'}\n",
      "{'loss': 0.26773865, 'acc': 0.91168432, 'grad_norm': 0.64449424, 'learning_rate': 9.01e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426585, 'epoch': 0.82, 'global_step/max_steps': '845/1036', 'percentage': '81.56%', 'elapsed_time': '33m 0s', 'remaining_time': '7m 27s'}\n",
      "{'loss': 0.26959395, 'acc': 0.91143026, 'grad_norm': 0.63826853, 'learning_rate': 8.56e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426699, 'epoch': 0.82, 'global_step/max_steps': '850/1036', 'percentage': '82.05%', 'elapsed_time': '33m 11s', 'remaining_time': '7m 15s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7258940fc6ec4c68a2dde964975d3017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25285795, 'eval_acc': 0.91247323, 'eval_runtime': 6.1494, 'eval_samples_per_second': 27.157, 'eval_steps_per_second': 27.157, 'epoch': 0.82, 'global_step/max_steps': '850/1036', 'percentage': '82.05%', 'elapsed_time': '33m 17s', 'remaining_time': '7m 17s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26363606, 'acc': 0.91079512, 'grad_norm': 0.65084207, 'learning_rate': 8.12e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42546, 'epoch': 0.83, 'global_step/max_steps': '855/1036', 'percentage': '82.53%', 'elapsed_time': '33m 29s', 'remaining_time': '7m 5s'}\n",
      "{'loss': 0.26936545, 'acc': 0.91184597, 'grad_norm': 0.71295285, 'learning_rate': 7.69e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425578, 'epoch': 0.83, 'global_step/max_steps': '860/1036', 'percentage': '83.01%', 'elapsed_time': '33m 40s', 'remaining_time': '6m 53s'}\n",
      "{'loss': 0.25182965, 'acc': 0.91700048, 'grad_norm': 0.58419657, 'learning_rate': 7.27e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425723, 'epoch': 0.83, 'global_step/max_steps': '865/1036', 'percentage': '83.49%', 'elapsed_time': '33m 51s', 'remaining_time': '6m 41s'}\n",
      "{'loss': 0.25947742, 'acc': 0.91516256, 'grad_norm': 0.6548515, 'learning_rate': 6.86e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425887, 'epoch': 0.84, 'global_step/max_steps': '870/1036', 'percentage': '83.98%', 'elapsed_time': '34m 2s', 'remaining_time': '6m 29s'}\n",
      "{'loss': 0.26497595, 'acc': 0.91130466, 'grad_norm': 0.6054911, 'learning_rate': 6.46e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426004, 'epoch': 0.84, 'global_step/max_steps': '875/1036', 'percentage': '84.46%', 'elapsed_time': '34m 13s', 'remaining_time': '6m 17s'}\n",
      "{'loss': 0.2768569, 'acc': 0.90655003, 'grad_norm': 0.59037399, 'learning_rate': 6.07e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426127, 'epoch': 0.85, 'global_step/max_steps': '880/1036', 'percentage': '84.94%', 'elapsed_time': '34m 24s', 'remaining_time': '6m 6s'}\n",
      "{'loss': 0.28850679, 'acc': 0.90699196, 'grad_norm': 0.58376086, 'learning_rate': 5.7e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426258, 'epoch': 0.85, 'global_step/max_steps': '885/1036', 'percentage': '85.42%', 'elapsed_time': '34m 35s', 'remaining_time': '5m 54s'}\n",
      "{'loss': 0.26699648, 'acc': 0.91416225, 'grad_norm': 0.65084869, 'learning_rate': 5.33e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42639, 'epoch': 0.86, 'global_step/max_steps': '890/1036', 'percentage': '85.91%', 'elapsed_time': '34m 46s', 'remaining_time': '5m 42s'}\n",
      "{'loss': 0.28879454, 'acc': 0.90830803, 'grad_norm': 0.58605325, 'learning_rate': 4.98e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426518, 'epoch': 0.86, 'global_step/max_steps': '895/1036', 'percentage': '86.39%', 'elapsed_time': '34m 57s', 'remaining_time': '5m 30s'}\n",
      "{'loss': 0.26799796, 'acc': 0.91118059, 'grad_norm': 0.70742047, 'learning_rate': 4.64e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426663, 'epoch': 0.87, 'global_step/max_steps': '900/1036', 'percentage': '86.87%', 'elapsed_time': '35m 8s', 'remaining_time': '5m 18s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674688ec4d2849758ec055360b4eba12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25273442, 'eval_acc': 0.91313023, 'eval_runtime': 6.2868, 'eval_samples_per_second': 26.564, 'eval_steps_per_second': 26.564, 'epoch': 0.87, 'global_step/max_steps': '900/1036', 'percentage': '86.87%', 'elapsed_time': '35m 15s', 'remaining_time': '5m 19s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26124253, 'acc': 0.91392221, 'grad_norm': 0.65191209, 'learning_rate': 4.31e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425367, 'epoch': 0.87, 'global_step/max_steps': '905/1036', 'percentage': '87.36%', 'elapsed_time': '35m 27s', 'remaining_time': '5m 7s'}\n",
      "{'loss': 0.27000453, 'acc': 0.9137394, 'grad_norm': 0.59691513, 'learning_rate': 3.99e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425486, 'epoch': 0.88, 'global_step/max_steps': '910/1036', 'percentage': '87.84%', 'elapsed_time': '35m 38s', 'remaining_time': '4m 56s'}\n",
      "{'loss': 0.2555125, 'acc': 0.913379, 'grad_norm': 0.6105693, 'learning_rate': 3.68e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425601, 'epoch': 0.88, 'global_step/max_steps': '915/1036', 'percentage': '88.32%', 'elapsed_time': '35m 49s', 'remaining_time': '4m 44s'}\n",
      "{'loss': 0.26615188, 'acc': 0.9119772, 'grad_norm': 0.63104069, 'learning_rate': 3.39e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425731, 'epoch': 0.89, 'global_step/max_steps': '920/1036', 'percentage': '88.80%', 'elapsed_time': '36m 0s', 'remaining_time': '4m 32s'}\n",
      "{'loss': 0.24242017, 'acc': 0.92146425, 'grad_norm': 0.64520019, 'learning_rate': 3.11e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425848, 'epoch': 0.89, 'global_step/max_steps': '925/1036', 'percentage': '89.29%', 'elapsed_time': '36m 11s', 'remaining_time': '4m 20s'}\n",
      "{'loss': 0.25870028, 'acc': 0.91415586, 'grad_norm': 0.62463033, 'learning_rate': 2.84e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425975, 'epoch': 0.9, 'global_step/max_steps': '930/1036', 'percentage': '89.77%', 'elapsed_time': '36m 22s', 'remaining_time': '4m 8s'}\n",
      "{'loss': 0.26573234, 'acc': 0.9105052, 'grad_norm': 0.69300294, 'learning_rate': 2.58e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426085, 'epoch': 0.9, 'global_step/max_steps': '935/1036', 'percentage': '90.25%', 'elapsed_time': '36m 33s', 'remaining_time': '3m 56s'}\n",
      "{'loss': 0.2576601, 'acc': 0.91479034, 'grad_norm': 0.64509505, 'learning_rate': 2.33e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426219, 'epoch': 0.91, 'global_step/max_steps': '940/1036', 'percentage': '90.73%', 'elapsed_time': '36m 44s', 'remaining_time': '3m 45s'}\n",
      "{'loss': 0.27704172, 'acc': 0.91140842, 'grad_norm': 0.61446726, 'learning_rate': 2.1e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426338, 'epoch': 0.91, 'global_step/max_steps': '945/1036', 'percentage': '91.22%', 'elapsed_time': '36m 56s', 'remaining_time': '3m 33s'}\n",
      "{'loss': 0.28045781, 'acc': 0.90948, 'grad_norm': 0.56329685, 'learning_rate': 1.87e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426483, 'epoch': 0.92, 'global_step/max_steps': '950/1036', 'percentage': '91.70%', 'elapsed_time': '37m 7s', 'remaining_time': '3m 21s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27890789c6cc4f0a90faf0b2b39658e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25269359, 'eval_acc': 0.91334923, 'eval_runtime': 6.3905, 'eval_samples_per_second': 26.133, 'eval_steps_per_second': 26.133, 'epoch': 0.92, 'global_step/max_steps': '950/1036', 'percentage': '91.70%', 'elapsed_time': '37m 13s', 'remaining_time': '3m 22s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26738656, 'acc': 0.9131752, 'grad_norm': 0.66077733, 'learning_rate': 1.66e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425347, 'epoch': 0.92, 'global_step/max_steps': '955/1036', 'percentage': '92.18%', 'elapsed_time': '37m 24s', 'remaining_time': '3m 10s'}\n",
      "{'loss': 0.26490004, 'acc': 0.91363659, 'grad_norm': 0.56394893, 'learning_rate': 1.46e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425466, 'epoch': 0.93, 'global_step/max_steps': '960/1036', 'percentage': '92.66%', 'elapsed_time': '37m 35s', 'remaining_time': '2m 58s'}\n",
      "{'loss': 0.25376015, 'acc': 0.91628246, 'grad_norm': 0.62034351, 'learning_rate': 1.28e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.42558, 'epoch': 0.93, 'global_step/max_steps': '965/1036', 'percentage': '93.15%', 'elapsed_time': '37m 47s', 'remaining_time': '2m 46s'}\n",
      "{'loss': 0.26298871, 'acc': 0.91119671, 'grad_norm': 0.59900969, 'learning_rate': 1.11e-06, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425673, 'epoch': 0.94, 'global_step/max_steps': '970/1036', 'percentage': '93.63%', 'elapsed_time': '37m 58s', 'remaining_time': '2m 35s'}\n",
      "{'loss': 0.2655066, 'acc': 0.91230097, 'grad_norm': 0.59715098, 'learning_rate': 9.5e-07, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425785, 'epoch': 0.94, 'global_step/max_steps': '975/1036', 'percentage': '94.11%', 'elapsed_time': '38m 9s', 'remaining_time': '2m 23s'}\n",
      "{'loss': 0.27306809, 'acc': 0.91232777, 'grad_norm': 0.6952616, 'learning_rate': 8e-07, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425925, 'epoch': 0.95, 'global_step/max_steps': '980/1036', 'percentage': '94.59%', 'elapsed_time': '38m 20s', 'remaining_time': '2m 11s'}\n",
      "{'loss': 0.255235, 'acc': 0.91693058, 'grad_norm': 0.64657986, 'learning_rate': 6.6e-07, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426071, 'epoch': 0.95, 'global_step/max_steps': '985/1036', 'percentage': '95.08%', 'elapsed_time': '38m 31s', 'remaining_time': '1m 59s'}\n",
      "{'loss': 0.25618062, 'acc': 0.91247215, 'grad_norm': 0.6781466, 'learning_rate': 5.4e-07, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426168, 'epoch': 0.96, 'global_step/max_steps': '990/1036', 'percentage': '95.56%', 'elapsed_time': '38m 42s', 'remaining_time': '1m 47s'}\n",
      "{'loss': 0.26707366, 'acc': 0.91347952, 'grad_norm': 0.58700734, 'learning_rate': 4.3e-07, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426296, 'epoch': 0.96, 'global_step/max_steps': '995/1036', 'percentage': '96.04%', 'elapsed_time': '38m 53s', 'remaining_time': '1m 36s'}\n",
      "{'loss': 0.28763397, 'acc': 0.90465565, 'grad_norm': 0.58786857, 'learning_rate': 3.3e-07, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.426457, 'epoch': 0.97, 'global_step/max_steps': '1000/1036', 'percentage': '96.53%', 'elapsed_time': '39m 4s', 'remaining_time': '1m 24s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c45c5189164836bbf6663725640e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25279331, 'eval_acc': 0.9131789, 'eval_runtime': 6.2974, 'eval_samples_per_second': 26.519, 'eval_steps_per_second': 26.519, 'epoch': 0.97, 'global_step/max_steps': '1000/1036', 'percentage': '96.53%', 'elapsed_time': '39m 10s', 'remaining_time': '1m 24s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.25318165, 'acc': 0.91595631, 'grad_norm': 0.65872234, 'learning_rate': 2.4e-07, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425337, 'epoch': 0.97, 'global_step/max_steps': '1005/1036', 'percentage': '97.01%', 'elapsed_time': '39m 22s', 'remaining_time': '1m 12s'}\n",
      "{'loss': 0.26736193, 'acc': 0.91522589, 'grad_norm': 0.64096749, 'learning_rate': 1.7e-07, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425438, 'epoch': 0.97, 'global_step/max_steps': '1010/1036', 'percentage': '97.49%', 'elapsed_time': '39m 33s', 'remaining_time': '1m 1s'}\n",
      "{'loss': 0.26323338, 'acc': 0.9125535, 'grad_norm': 0.58574092, 'learning_rate': 1.1e-07, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425547, 'epoch': 0.98, 'global_step/max_steps': '1015/1036', 'percentage': '97.97%', 'elapsed_time': '39m 44s', 'remaining_time': '49s'}\n",
      "{'loss': 0.25727088, 'acc': 0.9114131, 'grad_norm': 0.530321, 'learning_rate': 7e-08, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425658, 'epoch': 0.98, 'global_step/max_steps': '1020/1036', 'percentage': '98.46%', 'elapsed_time': '39m 55s', 'remaining_time': '37s'}\n",
      "{'loss': 0.27574167, 'acc': 0.90831127, 'grad_norm': 0.83077794, 'learning_rate': 3e-08, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425783, 'epoch': 0.99, 'global_step/max_steps': '1025/1036', 'percentage': '98.94%', 'elapsed_time': '40m 6s', 'remaining_time': '25s'}\n",
      "{'loss': 0.26432557, 'acc': 0.91130934, 'grad_norm': 0.72857958, 'learning_rate': 1e-08, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425881, 'epoch': 0.99, 'global_step/max_steps': '1030/1036', 'percentage': '99.42%', 'elapsed_time': '40m 18s', 'remaining_time': '14s'}\n",
      "{'loss': 0.29199944, 'acc': 0.90408525, 'grad_norm': 0.55515283, 'learning_rate': 0.0, 'memory(GiB)': 5.78, 'train_speed(iter/s)': 0.425984, 'epoch': 1.0, 'global_step/max_steps': '1035/1036', 'percentage': '99.90%', 'elapsed_time': '40m 29s', 'remaining_time': '2s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d275f8962474ebcb7c86c0eb77d54c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25266954, 'eval_acc': 0.9134709, 'eval_runtime': 6.2557, 'eval_samples_per_second': 26.695, 'eval_steps_per_second': 26.695, 'epoch': 1.0, 'global_step/max_steps': '1036/1036', 'percentage': '100.00%', 'elapsed_time': '40m 37s', 'remaining_time': '0s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-1036\n",
      "[INFO:swift] last_model_checkpoint: /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-1036\n",
      "[INFO:swift] best_model_checkpoint: /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/checkpoint-1036\n",
      "[INFO:swift] images_dir: /mnt/workspace/NLP_PJ-main/code/output_argu_2/qwen2_5-0_5b-instruct/v0-20241210-183550/images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2438.1581, 'train_samples_per_second': 6.799, 'train_steps_per_second': 0.425, 'train_loss': 0.27689182, 'epoch': 1.0, 'global_step/max_steps': '1036/1036', 'percentage': '100.00%', 'elapsed_time': '40m 38s', 'remaining_time': '0s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] End time of running main: 2024-12-10 19:17:01.835062\n"
     ]
    }
   ],
   "source": [
    "model_type = ModelType.qwen2_5_0_5b_instruct\n",
    "sft_args = SftArguments(\n",
    "    model_type=model_type,\n",
    "    dataset=['gsm8k_argu.jsonl'],\n",
    "    output_dir='output_argu_2',\n",
    "    max_length=4096)\n",
    "result = sft_main(sft_args)\n",
    "last_model_checkpoint = result['last_model_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
